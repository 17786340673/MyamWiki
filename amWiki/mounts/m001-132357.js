if(typeof AWPageMounts=='undefined'){AWPageMounts={}};AWPageMounts['m001']=[{"name":"01-Linux新建hadoop用户.md","path":"001-大数据/01-搭建hadoop平台/1-linux基础环境/01-Linux新建hadoop用户.md","content":"# <font color=#C71585>Linux新建hadoop用户</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n\r\n## 建立用户组\r\n```shell\r\n  [root@localhost ~]$ groupadd hadoop\r\n```\r\n## 新建hadoop用户并添加到用户组\r\n```shell\r\n [root@localhost ~]$ useradd -g hadoop hadoop\r\n [root@localhost ~]$ useradd -g hadoop hue\r\n```\r\n## 设置用户密码\r\n```shell\r\n[root@localhost ~]$ passwd hadoop   然后输入密码（密码）\r\n[root@localhost ~]$ passwd hue  然后输入密码（密码）\r\n```\r\n## 配置hadoop用户具有sudo权限\r\n```shell\r\n[root@localhost ~]$ usermod -G wheel hadoop\r\n[root@localhost ~]$ usermod -G wheel hue\r\n```\r\n## 配置hadoop用户sudo不需要密码\r\n```shell\r\n[hadoop@hadoop001 ~]$ sudo vim /etc/sudoers\r\n##添加\r\n##\thadoop  ALL=(ALL) NOPASSWD: ALL\r\n##放开限制\r\n##  ## Same thing without a password\r\n##\t%wheel  ALL=(ALL)       NOPASSWD: ALL\r\n##测试\r\n[hadoop@hadoop001 ~]$ sudo su - root\r\n##Last login: Wed Jan  5 16:51:55 CST 2022 on pts/1\r\n[root@hadoop001 ~]$ exit\r\n```\r\n## 在/opt目录下创建文件夹，并修改所属主和所属组\r\n```shell\r\n## 在/opt目录下创建module、software文件夹\r\n[root@test-hadoop001 ~]# mkdir /opt/module\r\n[root@test-hadoop001 ~]# mkdir /opt/software\r\n[root@test-hadoop001 ~]# mkdir /mnt/hadoop\r\n[root@test-hadoop001 ~]# mkdir /tmp/hive\r\n## 修改module、software文件夹的所有者和所属组均为hadoop用户\r\n[root@test-hadoop001 ~]# chown hadoop:hadoop /opt/module\r\n[root@test-hadoop001 ~]# chown hadoop:hadoop /opt/software\r\n[root@test-hadoop004 ~]# chown hadoop:hadoop /mnt/hadoop\r\n[root@test-hadoop004 ~]# chown hadoop:hadoop /tmp/hive\r\n```\r\n## 关闭防火墙，关闭防火墙开机自启\r\n```shell\r\n[root@localhost opt]$ systemctl stop firewalld\r\n[root@localhost opt]$ systemctl disable firewalld.service\r\n```\r\n## 查看防火墙状态\r\n```shell\r\n[hadoop@hadoop001 logs]$ firewall-cmd --state\r\n```\r\n## 重启机器\r\n```shell\r\n[root@localhost opt]$ reboot\r\n```\r\n## 安装telnet/vim\r\n```shell\r\n[hadoop@test-hadoop001 logs]$ sudo yum install telnet\r\n[hadoop@test-hadoop001 logs]$ sudo yum install vim*\r\n```\r\n","timestamp":1649836682854},{"name":"02-修改主机名称和ip映射.md","path":"001-大数据/01-搭建hadoop平台/1-linux基础环境/02-修改主机名称和ip映射.md","content":"# <font color=#C71585>修改主机名称和ip映射</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## 修改主机名称\r\n```shell\r\nsudo vim /etc/hostname\r\n\r\ntest-hadoop001(localhost.localdomain)\r\ntest-hadoop002(localhost.localdomain)\r\ntest-hadoop003(localhost.localdomain)\r\n```\r\n## 修改ip映射\r\n```shell\r\n[hadoop@localhost ~]$ sudo vim /etc/hosts\r\n192.168.20.201 test-hadoop001\r\n192.168.20.202 test-hadoop002\r\n192.168.20.203 test-hadoop003\r\n```\r\n## 重启3台机器\r\n```shell\r\n[hadoop@localhost ~]$ sudo su - root\r\n[root@localhost ~]$ reboot\r\n```\r\n","timestamp":1649836682854},{"name":"03-修改openfiles和max user processes.md","path":"001-大数据/01-搭建hadoop平台/1-linux基础环境/03-修改openfiles和max user processes.md","content":"# <font color=#C71585>修改openfiles和max user processes</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n```shell\r\n[hadoop@localhost ~]$ sudo vim  /etc/security/limits.d/20-nproc.conf\r\n*          soft    nproc     65535\r\nroot       soft    nproc     unlimited\r\n[hadoop@localhost ~]$ sudo vim /etc/security/limits.conf\r\nroot soft nofile 65535\r\nroot hard nofile 65535\r\n* soft nofile 65535\r\n* hard nofile 65535\r\n* soft noproc 65535\r\n* hard noproc 65535\r\n\r\n```\r\n*断开重连，ulimit -a验证是否修改成功*\r\n","timestamp":1649836682854},{"name":"04-Hadoop-SSH免密登录.md","path":"001-大数据/01-搭建hadoop平台/1-linux基础环境/04-Hadoop-SSH免密登录.md","content":"# <font color=#C71585>Hadoop-SSH免密登录</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## 每台机器执行，生成公钥和私钥\r\n```shell\r\n[hadoop@test-hadoop001 ~]$ ssh-keygen -t rsa\r\n[hadoop@test-hadoop001 ~]$ chmod 700 .ssh\r\n[hadoop@test-hadoop001 ~]$ chmod 600 authorized_keys\r\n```\r\n## 在test-Hadoop001执行\r\n```shell\r\n[hadoop@test-hadoop001 .ssh]$ cat id_rsa.pub >> authorized_keys\r\n[hadoop@test-hadoop001 .ssh]$ chmod 600 authorized_keys\r\n```\r\n## 将test-hadoop002、test-hadoop004的id_rsa.pub写入到hadoop001的authorized_keys文件中\r\n```shell\r\n[hadoop@test-hadoop001 .ssh]$ vim authorized_keys\r\n```\r\n## 密钥分发复制\r\n```shell\r\n[hadoop@test-hadoop002 .ssh]$ scp hadoop001:/home/hadoop/.ssh/authorized_keys /home/hadoop/.ssh/\r\n```\r\n","timestamp":1649836682854},{"name":"05-安装JAVA环境.md","path":"001-大数据/01-搭建hadoop平台/1-linux基础环境/05-安装JAVA环境.md","content":"# <font color=#C71585>安装JAVA环境</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## 从其他机器上复制java安装包\r\n```shell\r\n[hadoop@test-hadoop001 module]$ scp 192.168.20.206:/opt/software/jdk-8u212-linux-x64.tar.gz /opt/software/\r\n[hadoop@test-hadoop002 module]$ scp 192.168.20.181:/opt/software/jdk-8u212-linux-x64.tar.gz /opt/software/\r\n[hadoop@test-hadoop004 module]$ scp 192.168.20.181:/opt/software/jdk-8u212-linux-x64.tar.gz /opt/software/\r\n```\r\n## 解压安装包\r\n```shell\r\n[hadoop@test-hadoop001 module]$ tar -zxvf /opt/software/jdk-8u212-linux-x64.tar.gz -C /opt/module/\r\n```\r\n## 新建java配置文件\r\n```shell\r\nsudo vim /etc/profile.d/my_env.sh\r\n## JAVA_HOME\r\nexport JAVA_HOME=/opt/module/jdk1.8.0_212\r\nexport PATH=$PATH:$JAVA_HOME/bin\r\n[hadoop@test-hadoop001 module]$ source /etc/profile\r\n```\r\n## 验证java环境\r\n```shell\r\n[hadoop@test-hadoop001 module]$ java -version\r\n```\r\n","timestamp":1649836682854},{"name":"06-设置并同步时间-root用户.md","path":"001-大数据/01-搭建hadoop平台/1-linux基础环境/06-设置并同步时间-root用户.md","content":"# <font color=#C71585>设置并同步时间-root用户</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## 安装ntp\r\n```shell\r\nyum install -y ntp\r\n```\r\n## 与一个已知的时间服务器同步\r\n```shell\r\n# time.nist.gov 是一个时间服务器\r\n# time.nist.gov\r\n# time.nuri.net\r\n# 0.asia.pool.ntp.org\r\n# 1.asia.pool.ntp.org\r\n# 2.asia.pool.ntp.org\r\n# 3.asia.pool.ntp.org\r\nntpdate time.nist.gov\r\n```\r\n## 删除本地时间并设置时区为上海\r\n```shell\r\nrm -rf /etc/localtime\r\nln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\r\n```\r\n","timestamp":1649836682854},{"name":"07-释放buff-cache内存占用.md","path":"001-大数据/01-搭建hadoop平台/1-linux基础环境/07-释放buff-cache内存占用.md","content":"# <font color=#C71585>释放buff/cache内存占用</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n<div align=center>![alt文本](amWiki/images/Hadoop/buff.png \"buff/cache\")  \r\nbuff/cache</div>\r\n\r\n## hadoop用户编辑cache文件，并给可执行权限\r\n```\r\n[hadoop@hadoop001 logs]$ cd /opt/module/bin\r\n[hadoop@hadoop001 logs]$ vim  cache\r\n[hadoop@hadoop001 logs]$ 00 17 * * * echo 3 > /proc/sys/vm/drop_caches\r\n[root@hadoop001 bin]# chmod  777 cache\r\n```\r\n## 切换root用户\r\n```\r\n[hadoop@hadoop001 logs]$ sudo su – root\r\n[sudo] password for hadoop:\r\n```\r\n## 配置linux定时调度\r\n```\r\n[root@hadoop001 ~]# cd /opt/module/bin/\r\n[root@hadoop001 bin]# crontab   cache\r\n```\r\n## 查看调度信息\r\n```\r\n[root@hadoop001 bin]# crontab  -l\r\n```\r\n## 验证释放情况\r\n```\r\n[root@hadoop001 bin]# free -h\r\n```\r\n","timestamp":1649836682854},{"name":"01-hadoop搭建规划.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/01-hadoop搭建规划.md","content":"# <font color=#C71585>hadoop搭建规划</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## 方案选型\r\n\r\n| 应用名           | 版本             |\r\n| :-------------:  | :-------------: |\r\n| zookeeper        | 3.5.7           |\r\n| hadoop           | 3.1.3           |\r\n| tez              | 0.10.1          |\r\n| hive             | 3.1.2           |\r\n| spark            | 3.1.3           |\r\n| sqoop            | 1.4.7           |\r\n| dolphinscheduler | 1.3.8           |\r\n| maven            | 3.8.3           |\r\n| Python           | 3.7.0           |\r\n| hue              | 4.10.0          |\r\n| scala            | 2.12.15         |\r\n| java             | 1.8.0_212       |\r\n| kafka            | 2.12-3.0.0      |\r\n| flink            | 1.13.5          |\r\n| kafka-eagle      | 2.0.9           |\r\n|flink-streaming-platform-web | new  |\r\n| mysql            | 5.7以上         |\r\n\r\n## 机器配置\r\n\r\n| 机器名称         | 服务器名称     |CPU     |磁盘     |网络     |\r\n| :-------------:  | :-------------:  | :-------------:  | :-------------:  | :-------------:  |\r\n| hadoop001       | dell-1       | 20核 128G       | DDR4 固态(2T)       | 千兆网卡       |\r\n| hadoop002       | dell-2       | 20核 128G       | DDR4 固态(2T)       | 千兆网卡       |\r\n| hadoop003       | dell-3       | 32核 128G       | DDR3 固态(2T)       | 千兆网卡       |\r\n\r\n## 服务器规划\r\n\r\n| 组件名称 | 服务名称  | 进程名称 | 端口号 | 用途  | hadoop001 | hadoop002 | hadoop003 |\r\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\r\n| zookeeper | QuorumPeerMain | QuorumPeerMain | 2181 | 对客户端提供服务的端口 | ✔ | ✔ | ✔ |\r\n| hadoop | JournalNode-rpc | JournalNode | 8485 | RPC服务 | http://hadoop001:8485 | http://hadoop002:8485 | http://hadoop003:8485 |\r\n| hadoop | JournalNode-http | JournalNode | 8480 | HTTP服务 | http://hadoop001:8480 | http://hadoop002:8480 | http://hadoop003:8480 |\r\n| hadoop | NameNode-rpc | NameNode | 8020 | 接收Client连接的RPC端口，用于获取文件系统metadata信息 | http://hadoop001:8020 | http://hadoop002:8020 | ✖ |\r\n| hadoop | NameNode-http | NameNode | 9870 | HTTP服务 | http://hadoop001:9870 | http://hadoop002:9870 | ✖ |\r\n| hadoop | ZKFC| DFSZKFailoverController | 8019 | ZooKeeper FailoverController，用于NN HA | http://hadoop001:8019 | http://hadoop002:8019 | ✖ |\r\n| hadoop | DataNode| DataNode | 9864 | http服务的端口 | http://hadoop001:9864 | http://hadoop002:9864 | http://hadoop003:9864 |\r\n| hadoop | DataNode| DataNode | 9866 | datanode服务端口，用于数据传输 | http://hadoop001:9866 | http://hadoop002:9866 | http://hadoop003:9866 |\r\n| hadoop | DataNode| DataNode | 9867 | ipc服务的端口 | http://hadoop001:9867 | http://hadoop002:9867 | http://hadoop003:9867 |\r\n| hadoop | ResourceManager| ResourceManager | 8132 | RM的applications manager(ASM)端口 | http://hadoop001:8132 | http://hadoop002:8132 | ✖ |\r\n| hadoop | ResourceManager-scheduler| ResourceManager | 8130 | scheduler组件的IPC端口| http://hadoop001:8130 | http://hadoop002:8130 | ✖ |\r\n| hadoop | ResourceManager-web| ResourceManager | 8188 | http服务端口 | http://hadoop001:8188 | http://hadoop002:8188 |  |\r\n| hadoop | NodeManager| NodeManager | 8040 | http服务端口 | http://hadoop001:8040 | http://hadoop002:8040 | http://hadoop003:8040 |\r\n| hadoop | JobHistoryServer| JobHistoryServer | 10020 | IPC | ✖ | ✖ | http://hadoop003:10020 |\r\n| hadoop | JobHistoryServer web | JobHistoryServer web | 19888 | http服务端口 | ✖ | ✖ | http://hadoop003:19888 |\r\n| hive | HiveMetaStore | HiveMetaStore | 9083 | 元数据服务 | ✖ | ✖ | http://hadoop003:9083 |\r\n| hive | HiveServer2 | HiveServer2 | 10000 | jdbc连接服务 | ✖ | ✖ | http://hadoop003:10000 |\r\n| hive | HiveServer2-web | HiveServer2 | 10002 | http服务端口 | ✖ | ✖ | http://hadoop003:10002 |\r\n| Hue | Hue | Hue | 8888 | http服务端口 | http://Hadoop001:8888/hue | ✖ | ✖ |\r\n| dolphinscheduler | apiServerPort | apiServerPort | 12345 | http服务端口 | ✖ | http://hadoop002:12345/dolphinscheduler | ✖ |\r\n","timestamp":1649836682854},{"name":"02-环境变量配置.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/02-环境变量配置.md","content":"# <font color=#C71585>配置hadoop环境变量</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n# 编辑环境变量\r\n\r\n```\r\nsudo vim /etc/profile.d/my_env.sh\r\n```\r\n```\r\n#JAVA_HOME\r\nexport JAVA_HOME=/opt/module/jdk1.8.0_212\r\nexport PATH=$PATH:$JAVA_HOME/bin\r\n\r\n#xsync\r\nexport PATH=$PATH:/opt/module/bin\r\n\r\n#HADOOP_HOME\r\nexport HADOOP_HOME=/opt/module/hadoop\r\nexport PATH=$PATH:$HADOOP_HOME/bin\r\nexport PATH=$PATH:$HADOOP_HOME/sbin\r\nexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\r\nexport HADOOP_CLASSPATH=`hadoop classpath`\r\n\r\n#HIVE_HOME\r\nexport HIVE_HOME=/opt/module/hive\r\nexport PATH=$PATH:$HIVE_HOME/bin\r\n\r\n#SPARK_HOME\r\nexport SPARK_HOME=/opt/module/spark\r\nexport PATH=$PATH:$SPARK_HOME/bin\r\n\r\n#zookeeper\r\nexport ZK_HOME=/opt/module/zookeeper\r\nexport PATH=$PATH:$ZK_HOME/bin\r\n\r\n#maven\r\nexport M2_HOME=/opt/module/apache-maven-3.8.3\r\nexport PATH=$PATH:$M2_HOME/bin\r\n\r\n#node/npm\r\nexport NODE_HOME=/opt/module/node\r\nexport PATH=$PATH:$NODE_HOME/bin\r\n\r\n#scala\r\nexport SCALA_HOME=/opt/module/scala-2.12.15\r\nexport PATH=$PATH:$SCALA_HOME/bin\r\n\r\n#kafka-eagle\r\nexport KE_HOME=/opt/module/kafka-eagle-bin-2.0.9/efak-web-2.0.9\r\nexport PATH=$PATH:$KE_HOME/bin\r\n\r\n#flink\r\nexport FLINK_HOME=/opt/module/flink-1.13.5\r\nexport PATH=$PATH:$KE_HOME/bin\r\n\r\n```\r\n# 修改生效配置\r\n\r\n```\r\nsource /etc/profile\r\n```\r\n","timestamp":1649836682854},{"name":"03-linux机器同步命令xsync.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/03-linux机器同步命令xsync.md","content":"# <font color=#C71585>配置hadoop环境变量</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n\r\n\r\n# 新建xsync文件\r\n\r\n```\r\n[hadoop@hadoop001 ~]$ vim /opt/module/bin/xsync\r\n```\r\n``` java\r\n#!/bin/bash\r\n\r\n#1. 判断参数个数\r\nif [ $# -lt 1 ]\r\nthen\r\n   echo Not Enough Arguement!\r\n   exit;\r\nfi\r\n\r\n\r\n# 2遍历集群所有机器\r\nfor host in hadoop001 hadoop002 hadoop003\r\ndo\r\n   echo ==================== $host ====================\r\n   #3. 遍历所有目录，挨个发送\r\n   for file in $@\r\n   do\r\n     #4. 判断文件是否存在\r\n     if [ -e $file ]\r\n        then\r\n          #5. 获取父目录\r\n          pdir=$(cd -P $(dirname $file); pwd)\r\n          #6. 获取当前文件的名称\r\n          fname=$(basename $file)\r\n          ssh $host \"mkdir -p $pdir\"\r\n          rsync -av $pdir/$fname $host:$pdir\r\n        else\r\n          echo $file does not exists!\r\n    fi\r\n  done\r\ndone\r\n```\r\n\r\n# 修改脚本 xsync 具有执行权限\r\n\r\n```\r\nchmod +x xsync\r\n```\r\n\r\n# 同步环境变量配置（root所有者）\r\n\r\n```\r\n[hadoop@hadoop001 ~]$ sudo ./bin/xsync /etc/profile.d/my_env.sh\r\n```\r\n*注意：如果用了sudo，那么xsync一定要给它的路径补全。*\r\n\r\n# 让环境变量生效\r\n\r\n```\r\n[hadoop@hadoop002 bin]$ source /etc/profile\r\n[hadoop@hadoop003 bin]$ source /etc/profile\r\n```\r\n","timestamp":1649836682854},{"name":"04-zookeeper集群配置.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/04-zookeeper集群配置.md","content":"# <font color=#C71585>配置hadoop环境变量</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## 配置环境变量\r\n\r\n```\r\nsudo vim /etc/profile.d/my_env.sh\r\n```\r\n\r\n```\r\n#zookeeper\r\nexport ZK_HOME=/opt/module/zookeeper\r\nexport PATH=$PATH:$ZK_HOME/bin\r\n```\r\n\r\n## 文件分发\r\n```\r\n[hadoop@hadoop001 module]$ sudo /opt/module/bin/xsync /etc/profile.d/my_env.sh\r\n```\r\n## 让环境变量生效\r\n```\r\n[hadoop@hadoop002 bin]$ source /etc/profile\r\n[hadoop@hadoop003 bin]$ source /etc/profile\r\n```\r\n## 解压安装包并重命名\r\n```\r\n[hadoop@hadoop001 software]$ tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C ../module/\r\n[hadoop@hadoop001 module]$ mv zookeeper-3.5.7/ zookeeper\r\n```\r\n\r\n## 在zookeeper下新建文件夹logs和data\r\n```\r\n[hadoop@hadoop001 zookeeper]$ mkdir -p data logs\r\n```\r\n## 修改配置文件zoo.cfg\r\n```\r\n[hadoop@hadoop001 conf]$ cp zoo_sample.cfg zoo.cfg\r\n[hadoop@hadoop001 conf]$ vim zoo.cfg\r\n```\r\n``` js\r\n# The number of milliseconds of each tick\r\ntickTime=2000\r\n# The number of ticks that the initial\r\n# synchronization phase can take\r\ninitLimit=5\r\n# The number of ticks that can pass between\r\n# sending a request and getting an acknowledgement\r\nsyncLimit=6\r\n# the directory where the snapshot is stored.\r\n# do not use /tmp for storage, /tmp here is just\r\n# example sakes.\r\ndataDir=/opt/module/zookeeper/data\r\ndataLogDir=/opt/module/zookeeper/logs\r\n# the port at which the clients will connect\r\nclientPort=2181\r\n# the maximum number of client connections.\r\n# increase this if you need to handle more clients\r\nmaxClientCnxns=300\r\n#\r\n# Be sure to read the maintenance section of the\r\n# administrator guide before turning on autopurge.\r\n#\r\n# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance\r\n#\r\n# The number of snapshots to retain in dataDir\r\n#autopurge.snapRetainCount=3\r\n# Purge task interval in hours\r\n# Set to \"0\" to disable auto purge feature\r\n#autopurge.purgeInterval=1\r\nserver.1=hadoop001:2888:3888\r\nserver.2=hadoop002:2888:3888\r\nserver.3=hadoop003:2888:3888\r\n```\r\n\r\n##\t在data目录下新建myid，与配置一致即可\r\n```\r\n[hadoop@hadoop001 data]$ echo \"1\" > myid\r\n```\r\n## 分发到各个节点并修改myid文件为对应的编号\r\n```\r\n[hadoop@hadoop001 module]$ xsync zookeeper/\r\n```\r\n## 启动zookeeper服务\r\n```\r\n1.查看各个服务状态：./zkServer.sh status  \r\n2.重启zookeeper服务：./zkServer.sh restart  \r\n3.关闭zookeeper服务：./zkServer.sh stop  \r\n4.启动zookeeper服务：./zkServer.sh start(三台均需要执行)\r\n```\r\n","timestamp":1649836682854},{"name":"05-hadoop核心配置-yarn.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/05-hadoop核心配置-yarn.md","content":"# <font color=#C71585>hadoop核心配置-yarn</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## hadoop/etc/hadoop/yarn-site.xml\r\n\r\n```xml\r\n<?xml version=\"1.0\"?>\r\n<!--\r\n  Licensed under the Apache License, Version 2.0 (the \"License\");\r\n  you may not use this file except in compliance with the License.\r\n  You may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n  Unless required by applicable law or agreed to in writing, software\r\n  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n  See the License for the specific language governing permissions and\r\n  limitations under the License. See accompanying LICENSE file.\r\n-->\r\n<configuration>\r\n  <!--日志聚合功能-->\r\n  <property>\r\n     <name>yarn.log-aggregation-enable</name>\r\n     <value>true</value>\r\n  </property>\r\n\r\n<!-- 设置日志聚集服务器地址 -->\r\n<property>\r\n    <name>yarn.log.server.url</name>\r\n    <value>http://hadoop001:19888/jobhistory/logs</value>\r\n</property>\r\n\r\n  <!--在HDFS上聚合的日志最长保留多少秒。3天-->\r\n  <property>\r\n     <name>yarn.log-aggregation.retain-seconds</name>\r\n     <value>259200</value>\r\n  </property>\r\n\r\n  <!--rm失联后重新链接的时间-->\r\n  <property>\r\n     <name>yarn.resourcemanager.connect.retry-interval.ms</name>\r\n     <value>2000</value>\r\n  </property>\r\n\r\n  <!--开启resource manager HA,默认为false-->\r\n  <property>\r\n     <name>yarn.resourcemanager.ha.enabled</name>\r\n     <value>true</value>\r\n  </property>\r\n\r\n  <!--配置resource manager -->\r\n  <property>\r\n    <name>yarn.resourcemanager.ha.rm-ids</name>\r\n    <value>rm1,rm2</value>\r\n  </property>\r\n\r\n  <!--zookeeper HA-->\r\n  <property>\r\n    <name>ha.zookeeper.quorum</name>\r\n    <value>hadoop001:2181,hadoop002:2181,hadoop003:2181</value>\r\n  </property>\r\n\r\n  <!--开启故障自动切换-->\r\n  <property>\r\n     <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>\r\n     <value>true</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.resourcemanager.hostname.rm1</name>\r\n    <value>hadoop001</value>\r\n  </property>\r\n\r\n  <property>\r\n     <name>yarn.resourcemanager.hostname.rm2</name>\r\n     <value>hadoop002</value>\r\n  </property>\r\n\r\n  <!--在namenode1上配置rm1,在namenode2上配置rm2,注意：一般都喜欢把配置好的文件远程复制到其它机器上，但这个在YARN的另一个机器上一定要修改-->\r\n  <property>\r\n    <name>yarn.resourcemanager.ha.id</name>\r\n    <value>rm1</value>\r\n  </property>\r\n\r\n  <!--开启自动恢复功能-->\r\n  <property>\r\n    <name>yarn.resourcemanager.recovery.enabled</name>\r\n    <value>true</value>\r\n  </property>\r\n\r\n  <!--配置与zookeeper的连接地址-->\r\n  <property>\r\n    <name>yarn.resourcemanager.zk-state-store.address</name>\r\n    <value>hadoop001:2181,hadoop002:2181,hadoop003:2181</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.resourcemanager.store.class</name>\r\n    <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.resourcemanager.zk-address</name>\r\n    <value>hadoop001:2181,hadoop002:2181,hadoop003:2181</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.resourcemanager.cluster-id</name>\r\n    <value>x6mccluster-yarn</value>\r\n  </property>\r\n\r\n  <!-- Container申请资源最低要求内存-->\r\n  <property>\r\n    <name>yarn.scheduler.minimum-allocation-mb</name>\r\n    <value>1024</value>\r\n  </property>\r\n\r\n  <!-- Container申请资源最大要求内存-->\r\n  <property>\r\n    <name>yarn.scheduler.maximum-allocation-mb</name>\r\n    <value>4096</value>\r\n  </property>\r\n\r\n  <!--schelduler失联等待连接时间-->\r\n  <property>\r\n    <name>yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms</name>\r\n    <value>5000</value>\r\n  </property>\r\n\r\n  <!--每个NodeManager CPU个数-->\r\n  <property>\r\n    <name>yarn.nodemanager.resource.cpu-vcores</name>\r\n    <value>12</value>\r\n  </property>\r\n\r\n  <!--每个NodeManager 总内存-->\r\n  <property>\r\n    <name>yarn.nodemanager.resource.memory-mb</name>\r\n    <value>12288</value>\r\n   </property>\r\n\r\n   <!--单任务最少vcore-->\r\n  <property>\r\n    <name>yarn.scheduler.minimum-allocation-vcores</name>\r\n    <value>1</value>\r\n   </property>\r\n\r\n   <!--单任务最多vcore-->\r\n  <property>\r\n    <name>yarn.scheduler.maximum-allocation-vcores</name>\r\n    <value>4</value>\r\n   </property>\r\n\r\n  <!--配置rm1-->\r\n  <property>\r\n    <name>yarn.resourcemanager.address.rm1</name>\r\n    <value>hadoop001:8132</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.resourcemanager.scheduler.address.rm1</name>\r\n    <value>hadoop001:8130</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.resourcemanager.webapp.address.rm1</name>\r\n    <value>hadoop001:8188</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.resourcemanager.resource-tracker.address.rm1</name>\r\n    <value>hadoop001:8131</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.resourcemanager.admin.address.rm1</name>\r\n    <value>hadoop001:8033</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.resourcemanager.ha.admin.address.rm1</name>\r\n    <value>hadoop001:23142</value>\r\n  </property>\r\n\r\n  <!--配置rm2-->\r\n  <property>\r\n    <name>yarn.resourcemanager.address.rm2</name>\r\n    <value>hadoop002:8132</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.resourcemanager.scheduler.address.rm2</name>\r\n    <value>hadoop002:8130</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.resourcemanager.webapp.address.rm2</name>\r\n    <value>hadoop002:8188</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.resourcemanager.resource-tracker.address.rm2</name>\r\n    <value>hadoop002:8131</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.resourcemanager.admin.address.rm2</name>\r\n    <value>hadoop002:8033</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.resourcemanager.ha.admin.address.rm2</name>\r\n    <value>hadoop002:23142</value>\r\n  </property>\r\n\r\n\r\n  <!-- spark 动态资源分配 -->\r\n  <property>\r\n    <name>yarn.nodemanager.aux-services</name>\r\n    <value>mapreduce_shuffle,spark_shuffle</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>\r\n    <value>org.apache.spark.network.yarn.YarnShuffleService</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>spark.shuffle.service.port</name>\r\n    <value>7337</value>\r\n  </property>\r\n\r\n  <!-- spark 部署到 yarn 上需要这两个配置 -->\r\n  <!-- 是否启动一个线程检查每个任务正在使用的物理内存，如果超出分配值，则直接杀掉该任务，默认为 true -->\r\n  <property>\r\n    <name>yarn.nodemanager.pmem-check-enabled</name>\r\n    <value>false</value>\r\n  </property>\r\n\r\n  <!-- 是否启动一个线程检查每个任务正在试用的虚拟内存，如果超出分配值，则直接杀掉该任务，默认为 true -->\r\n  <property>\r\n    <name>yarn.nodemanager.vmem-check-enabled</name>\r\n    <value>false</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\r\n    <value>org.apache.hadoop.mapred.ShuffleHandler</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.nodemanager.local-dirs</name>\r\n    <value>/mnt/hadoop/yarn/local</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.nodemanager.log-dirs</name>\r\n    <value>/share/opt/module/hadoop/logs/yarn</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>mapreduce.shuffle.port</name>\r\n    <value>23080</value>\r\n  </property>\r\n\r\n  <!--故障处理类-->\r\n  <property>\r\n    <name>yarn.client.failover-proxy-provider</name>\r\n    <value>org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider</value>\r\n  </property>\r\n\r\n  <property>\r\n      <name>yarn.resourcemanager.ha.automatic-failover.zk-base-path</name>\r\n      <value>/yarn-leader-election</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.resourcemanager.scheduler.class</name>\r\n    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>\r\n  </property>\r\n\r\n  <!-- Yarn环境加载 -->\r\n  <property>\r\n        <name>yarn.application.classpath</name>\r\n        <value>\r\n            /share/opt/module/hadoop/etc/hadoop,\r\n            /share/opt/module/hadoop/share/hadoop/common/*,\r\n            /share/opt/module/hadoop/share/hadoop/common/lib/*,\r\n            /share/opt/module/hadoop/share/hadoop/hdfs/*,\r\n            /share/opt/module/hadoop/share/hadoop/hdfs/lib/*,\r\n            /share/opt/module/hadoop/share/hadoop/yarn/*,\r\n            /share/opt/module/hadoop/share/hadoop/yarn/lib/*,\r\n            /share/opt/module/hadoop/share/hadoop/mapreduce/*,\r\n            /share/opt/module/hadoop/share/hadoop/mapreduce/lib/*,\r\n            /share/opt/module/hadoop/contrib/capacity-scheduler/*.jar\r\n        </value>\r\n  </property>\r\n\r\n</configuration>\r\n```\r\n","timestamp":1649836682854},{"name":"06-hadoop核心配置-tez.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/06-hadoop核心配置-tez.md","content":"# <font color=#C71585>hadoop核心配置-tez</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## hadoop/etc/hadoop/tez-site.xml\r\n\r\n```xml\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\r\n\r\n<configuration>\r\n  <!-- 这里指向hdfs上的tez.tar.gz包 -->  \r\n  <property>  \r\n    <name>tez.lib.uris</name>  \r\n    <value>${fs.defaultFS}/tez/tez.tar.gz</value>\r\n  </property>\r\n</configuration>\r\n```\r\n","timestamp":1649836682854},{"name":"07-hadoop核心配置-mapred.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/07-hadoop核心配置-mapred.md","content":"# <font color=#C71585>hadoop核心配置-mapred</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## hadoop/etc/hadoop/mapred-site.xml\r\n\r\n```xml\r\n<?xml version=\"1.0\"?>\r\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\r\n<!--\r\n  Licensed under the Apache License, Version 2.0 (the \"License\");\r\n  you may not use this file except in compliance with the License.\r\n  You may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n  Unless required by applicable law or agreed to in writing, software\r\n  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n  See the License for the specific language governing permissions and\r\n  limitations under the License. See accompanying LICENSE file.\r\n-->\r\n\r\n<!-- Put site-specific property overrides in this file. -->\r\n\r\n<configuration>\r\n\r\n  <!-- 配置MapReduce运行于yarn中 -->\r\n  <property>\r\n    <name>mapreduce.framework.name</name>\r\n    <value>yarn</value>\r\n  </property>\r\n\r\n  <!-- 配置 MapReduce JobHistory Server 地址 ，默认端口10020 -->\r\n  <property>\r\n    <name>mapreduce.jobhistory.address</name>\r\n    <value>hadoop001:10020</value>\r\n  </property>\r\n\r\n  <!-- 配置 MapReduce JobHistory Server web ui 地址， 默认端口19888 -->\r\n  <property>\r\n    <name>mapreduce.jobhistory.webapp.address</name>\r\n    <value>hadoop001:19888</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.app.mapreduce.am.env</name>\r\n    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>mapreduce.map.env</name>\r\n    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>mapreduce.reduce.env</name>\r\n    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\r\n  </property>\r\n\r\n  <!-- 同时整理文件,这就决定了打开的文件句柄数 -->\r\n  <property>\r\n    <name>mapreduce.task.io.sort.factor</name>\r\n    <value>30</value>\r\n  </property>\r\n\r\n  <!-- 缓冲存储器的总量，同时整理文件所用内存 -->\r\n  <property>\r\n    <name>mapreduce.task.io.sort.mb</name>\r\n    <value>384</value>\r\n  </property>\r\n\r\n  <!-- reduce端并行获取线程数 -->\r\n  <property>\r\n    <name>mapreduce.reduce.shuffle.parallelcopies</name>\r\n    <value>10</value>\r\n  </property>\r\n\r\n  <!-- 内存量从调度器为每个Map申请的要求 -->\r\n  <property>\r\n    <name>mapreduce.map.memory.mb</name>\r\n    <value>1024</value>\r\n  </property>\r\n\r\n  <!-- 内存量从调度器为每个Reduce申请的要求 -->\r\n  <property>\r\n    <name>mapreduce.reduce.memory.mb</name>\r\n    <value>2048</value>\r\n  </property>\r\n\r\n  <!-- map内存大小 -->\r\n  <property>\r\n    <name>mapreduce.map.java.opts</name>\r\n    <value>-Xmx1536m</value>\r\n  </property>\r\n\r\n  <!-- reduce内存大小 -->\r\n  <property>\r\n    <name>mapreduce.reduce.java.opts</name>\r\n    <value>-Xmx2048m</value>\r\n  </property>\r\n\r\n  <!-- 每服务器允许启动的最大map槽位数 -->\r\n  <property>\r\n    <name>mapreduce.tasktracker.map.tasks.maximum</name>\r\n    <value>10</value>\r\n  </property>\r\n\r\n  <!-- 每服务器允许启动的最大reduce槽位数 -->\r\n  <property>\r\n    <name>mapreduce.tasktracker.reduce.tasks.maximum</name>\r\n    <value>10</value>\r\n  </property>\r\n\r\n  <!-- reduce使用CPU个数 -->\r\n  <property>\r\n    <name>mapreduce.reduce.cpu.vcores</name>\r\n    <value>1</value>\r\n  </property>\r\n\r\n  <!-- 本地运算文件夹剩余空间低于1G值则不在本地做计算 -->\r\n  <property>\r\n    <name>mapreduce.tasktracker.local.dir.minspacestart</name>\r\n    <value>1073741824</value>\r\n  </property>\r\n\r\n  <!-- 本地计算文件夹剩余空间低于1G值则不再申请新的任务,并清理已完成任务 -->\r\n  <property>\r\n    <name>mapreduce.tasktracker.local.dir.minspacekill</name>\r\n    <value>1073741824</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>mapreduce.shuffle.port</name>\r\n    <value>23080</value>\r\n  </property>\r\n\r\n  <!-- MapReduce做本地计算所使用的文件夹,为了分散磁盘I / O用逗号分隔在不同设备上的目录列表 -->\r\n  <property>\r\n    <name>mapreduce.cluster.local.dir</name>\r\n    <value>/mnt/hadoop/mapred/local</value>\r\n  </property>\r\n\r\n</configuration>\r\n\r\n```\r\n","timestamp":1649836682854},{"name":"08-hadoop核心配置-hdfs.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/08-hadoop核心配置-hdfs.md","content":"# <font color=#C71585>hadoop核心配置-hdfs</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## hadoop/etc/hadoop/hdfs-site.xml\r\n\r\n```xml\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\r\n<!--\r\n  Licensed under the Apache License, Version 2.0 (the \"License\");\r\n  you may not use this file except in compliance with the License.\r\n  You may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n  Unless required by applicable law or agreed to in writing, software\r\n  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n  See the License for the specific language governing permissions and\r\n  limitations under the License. See accompanying LICENSE file.\r\n-->\r\n\r\n<!-- Put site-specific property overrides in this file. -->\r\n\r\n<configuration>\r\n  <!--指定hdfs的block大小64M -->\r\n  <property>\r\n    <name>dfs.blocksize</name>\r\n    <value>134217728</value>\r\n  </property>\r\n\r\n  <!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 -->\r\n  <property>\r\n    <name>dfs.nameservices</name>\r\n    <value>x6mccluster</value>\r\n  </property>\r\n\r\n  <!-- ns1下面有两个NameNode，分别是nn1，nn2 -->\r\n  <property>\r\n    <name>dfs.ha.namenodes.x6mccluster</name>\r\n    <value>nn1,nn2</value>\r\n  </property>\r\n\r\n  <!-- nn1的RPC通信地址 -->\r\n  <property>\r\n    <name>dfs.namenode.rpc-address.x6mccluster.nn1</name>\r\n    <value>hadoop001:8020</value>\r\n  </property>\r\n\r\n  <!-- nn1的http通信地址 -->\r\n  <property>\r\n    <name>dfs.namenode.http-address.x6mccluster.nn1</name>\r\n    <value>hadoop001:9870</value>\r\n  </property>\r\n\r\n  <!-- nn2的RPC通信地址 -->\r\n  <property>\r\n    <name>dfs.namenode.rpc-address.x6mccluster.nn2</name>\r\n    <value>hadoop002:8020</value>\r\n  </property>\r\n\r\n  <!-- nn2的http通信地址 -->\r\n  <property>\r\n    <name>dfs.namenode.http-address.x6mccluster.nn2</name>\r\n    <value>hadoop002:9870</value>\r\n  </property>\r\n\r\n  <!-- 指定NameNode的元数据在JournalNode上的存放位置 -->\r\n  <property>\r\n    <name>dfs.namenode.shared.edits.dir</name>\r\n    <value>qjournal://hadoop001:8485;hadoop002:8485;hadoop003:8485/x6mccluster</value>\r\n  </property>\r\n\r\n  <!-- 配置失败自动切换实现方式 -->\r\n  <property>\r\n    <name>dfs.client.failover.proxy.provider.x6mccluster</name>\r\n    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\r\n  </property>\r\n\r\n  <!-- 配置隔离机制 -->\r\n  <property>\r\n    <name>dfs.ha.fencing.methods</name>\r\n    <value>sshfence</value>\r\n  </property>\r\n\r\n  <!-- 使用隔离机制时需要ssh免密码登陆 -->\r\n  <property>\r\n    <name>dfs.ha.fencing.ssh.private-key-files</name>\r\n    <value>/home/hadoop/.ssh/id_rsa</value>\r\n  </property>\r\n\r\n  <!-- 指定NameNode的元数据在JournalNode上的存放位置 -->\r\n  <property>\r\n    <name>dfs.journalnode.edits.dir</name>\r\n    <value>/mnt/hadoop/journal</value>\r\n  </property>\r\n\r\n  <!--指定支持高可用自动切换机制-->\r\n  <property>\r\n    <name>dfs.ha.automatic-failover.enabled</name>\r\n    <value>true</value>\r\n  </property>\r\n\r\n  <!--指定namenode名称空间的存储地址-->\r\n  <property>\r\n    <name>dfs.namenode.name.dir</name>\r\n    <value>/mnt/hadoop/dfs/nn</value>\r\n  </property>\r\n\r\n  <!--指定datanode数据存储地址-->\r\n  <property>\r\n    <name>dfs.datanode.data.dir</name>\r\n    <value>/mnt/hadoop/dfs/dn</value>\r\n  </property>\r\n\r\n  <!--指定数据冗余份数，设置数据块应该被复制的份数-->\r\n  <property>\r\n    <name>dfs.replication</name>\r\n    <value>2</value>\r\n  </property>\r\n\r\n  <!--设定数据块副本的最小份数-->\r\n  <property>\r\n    <name>dfs.namenode.replication.min</name>\r\n    <value>1</value>\r\n  </property>\r\n\r\n  <!--指定可以通过web访问hdfs目录-->\r\n  <property>\r\n    <name>dfs.webhdfs.enabled</name>\r\n    <value>true</value>\r\n  </property>\r\n\r\n  <!--工作线程池用来处理客户端的远程过程调用及集群守护进程的调用-->\r\n  <property>\r\n    <name>dfs.namenode.handler.count</name>\r\n    <value>30</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>dfs.datanode.handler.count</name>\r\n    <value>50</value>\r\n  </property>\r\n\r\n  <!--保证数据恢复 -->\r\n  <property>\r\n    <name>dfs.journalnode.http-address</name>\r\n    <value>0.0.0.0:8480</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>dfs.journalnode.rpc-address</name>\r\n    <value>0.0.0.0:8485</value>\r\n  </property>\r\n\r\n  <!--zookeeper HA-->\r\n  <property>\r\n    <name>ha.zookeeper.quorum</name>\r\n    <value>hadoop001:2181,hadoop002:2181,hadoop003:2181</value>\r\n  </property>\r\n\r\n  <!--权限检查 -->\r\n  <property>\r\n    <name>dfs.permissions.enabled</name>\r\n    <value>true</value>\r\n  </property>\r\n\r\n  <!--超级用户组名 -->\r\n  <property>\r\n    <name>dfs.permissions.supergroup</name>\r\n    <value>hadoop</value>\r\n  </property>\r\n\r\n</configuration>\r\n\r\n```\r\n","timestamp":1649836682854},{"name":"09-hadoop核心配置-env.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/09-hadoop核心配置-env.md","content":"# <font color=#C71585>hadoop核心配置-env</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## hadoop/etc/hadoop/hadoop-env.sh\r\n\r\n```xml\r\n#\r\n# Licensed to the Apache Software Foundation (ASF) under one\r\n# or more contributor license agreements.  See the NOTICE file\r\n# distributed with this work for additional information\r\n# regarding copyright ownership.  The ASF licenses this file\r\n# to you under the Apache License, Version 2.0 (the\r\n# \"License\"); you may not use this file except in compliance\r\n# with the License.  You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n# Set Hadoop-specific environment variables here.\r\n\r\n##\r\n## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.\r\n## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,\r\n## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE\r\n## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.\r\n##\r\n## Precedence rules:\r\n##\r\n## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults\r\n##\r\n## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults\r\n##\r\n\r\n# Many of the options here are built from the perspective that users\r\n# may want to provide OVERWRITING values on the command line.\r\n# For example:\r\n#\r\n#  JAVA_HOME=/usr/java/testing hdfs dfs -ls\r\n#\r\n# Therefore, the vast majority (BUT NOT ALL!) of these defaults\r\n# are configured for substitution and not append.  If append\r\n# is preferable, modify this file accordingly.\r\n\r\n###\r\n# Generic settings for HADOOP\r\n###\r\n\r\n# Technically, the only required environment variable is JAVA_HOME.\r\n# All others are optional.  However, the defaults are probably not\r\n# preferred.  Many sites configure these options outside of Hadoop,\r\n# such as in /etc/profile.d\r\n\r\n# The java implementation to use. By default, this environment\r\n# variable is REQUIRED on ALL platforms except OS X!\r\n# export JAVA_HOME=\r\n\r\n# Location of Hadoop.  By default, Hadoop will attempt to determine\r\n# this location based upon its execution path.\r\n# export HADOOP_HOME=\r\n\r\n# Location of Hadoop\'s configuration information.  i.e., where this\r\n# file is living. If this is not defined, Hadoop will attempt to\r\n# locate it based upon its execution path.\r\n#\r\n# NOTE: It is recommend that this variable not be set here but in\r\n# /etc/profile.d or equivalent.  Some options (such as\r\n# --config) may react strangely otherwise.\r\n#\r\n# export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\r\n\r\n# The maximum amount of heap to use (Java -Xmx).  If no unit\r\n# is provided, it will be converted to MB.  Daemons will\r\n# prefer any Xmx setting in their respective _OPT variable.\r\n# There is no default; the JVM will autoscale based upon machine\r\n# memory size.\r\n# export HADOOP_HEAPSIZE_MAX=\r\n\r\n# The minimum amount of heap to use (Java -Xms).  If no unit\r\n# is provided, it will be converted to MB.  Daemons will\r\n# prefer any Xms setting in their respective _OPT variable.\r\n# There is no default; the JVM will autoscale based upon machine\r\n# memory size.\r\n# export HADOOP_HEAPSIZE_MIN=\r\n\r\n# Enable extra debugging of Hadoop\'s JAAS binding, used to set up\r\n# Kerberos security.\r\n# export HADOOP_JAAS_DEBUG=true\r\n\r\n# Extra Java runtime options for all Hadoop commands. We don\'t support\r\n# IPv6 yet/still, so by default the preference is set to IPv4.\r\n# export HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true\"\r\n# For Kerberos debugging, an extended option set logs more invormation\r\n# export HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug\"\r\n\r\n# Some parts of the shell code may do special things dependent upon\r\n# the operating system.  We have to set this here. See the next\r\n# section as to why....\r\nexport HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}\r\n\r\n# Extra Java runtime options for some Hadoop commands\r\n# and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for\r\n# such commands.  In most cases, # this should be left empty and\r\n# let users supply it on the command line.\r\nexport HADOOP_CLIENT_OPTS=\"-Xmx4096m $HADOOP_CLIENT_OPTS\"\r\n\r\n#\r\n# A note about classpaths.\r\n#\r\n# By default, Apache Hadoop overrides Java\'s CLASSPATH\r\n# environment variable.  It is configured such\r\n# that it sarts out blank with new entries added after passing\r\n# a series of checks (file/dir exists, not already listed aka\r\n# de-deduplication).  During de-depulication, wildcards and/or\r\n# directories are *NOT* expanded to keep it simple. Therefore,\r\n# if the computed classpath has two specific mentions of\r\n# awesome-methods-1.0.jar, only the first one added will be seen.\r\n# If two directories are in the classpath that both contain\r\n# awesome-methods-1.0.jar, then Java will pick up both versions.\r\n\r\n# An additional, custom CLASSPATH. Site-wide configs should be\r\n# handled via the shellprofile functionality, utilizing the\r\n# hadoop_add_classpath function for greater control and much\r\n# harder for apps/end-users to accidentally override.\r\n# Similarly, end users should utilize ${HOME}/.hadooprc .\r\n# This variable should ideally only be used as a short-cut,\r\n# interactive way for temporary additions on the command line.\r\n# export HADOOP_CLASSPATH=\"/some/cool/path/on/your/machine\"\r\n\r\n# Should HADOOP_CLASSPATH be first in the official CLASSPATH?\r\n# export HADOOP_USER_CLASSPATH_FIRST=\"yes\"\r\n\r\n# If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along\r\n# with the main jar are handled by a separate isolated\r\n# client classloader when \'hadoop jar\', \'yarn jar\', or \'mapred job\'\r\n# is utilized. If it is set, HADOOP_CLASSPATH and\r\n# HADOOP_USER_CLASSPATH_FIRST are ignored.\r\n# export HADOOP_USE_CLIENT_CLASSLOADER=true\r\n\r\n# HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of\r\n# system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER\r\n# is enabled. Names ending in \'.\' (period) are treated as package names, and\r\n# names starting with a \'-\' are treated as negative matches. For example,\r\n# export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES=\"-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop.\"\r\n\r\n# Enable optional, bundled Hadoop features\r\n# This is a comma delimited list.  It may NOT be overridden via .hadooprc\r\n# Entries may be added/removed as needed.\r\n# export HADOOP_OPTIONAL_TOOLS=\"hadoop-kafka,hadoop-aws,hadoop-aliyun,hadoop-openstack,hadoop-azure,hadoop-azure-datalake\"\r\n\r\n###\r\n# Options for remote shell connectivity\r\n###\r\n\r\n# There are some optional components of hadoop that allow for\r\n# command and control of remote hosts.  For example,\r\n# start-dfs.sh will attempt to bring up all NNs, DNS, etc.\r\n\r\n# Options to pass to SSH when one of the \"log into a host and\r\n# start/stop daemons\" scripts is executed\r\n# export HADOOP_SSH_OPTS=\"-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s\"\r\n\r\n# The built-in ssh handler will limit itself to 10 simultaneous connections.\r\n# For pdsh users, this sets the fanout size ( -f )\r\n# Change this to increase/decrease as necessary.\r\n# export HADOOP_SSH_PARALLEL=10\r\n\r\n# Filename which contains all of the hosts for any remote execution\r\n# helper scripts # such as workers.sh, start-dfs.sh, etc.\r\n# export HADOOP_WORKERS=\"${HADOOP_CONF_DIR}/workers\"\r\n\r\n###\r\n# Options for all daemons\r\n###\r\n#\r\n\r\n#\r\n# Many options may also be specified as Java properties.  It is\r\n# very common, and in many cases, desirable, to hard-set these\r\n# in daemon _OPTS variables.  Where applicable, the appropriate\r\n# Java property is also identified.  Note that many are re-used\r\n# or set differently in certain contexts (e.g., secure vs\r\n# non-secure)\r\n#\r\n\r\n# Where (primarily) daemon log files are stored.\r\n# ${HADOOP_HOME}/logs by default.\r\n# Java property: hadoop.log.dir\r\n# export HADOOP_LOG_DIR=${HADOOP_HOME}/logs\r\n\r\n# A string representing this instance of hadoop. $USER by default.\r\n# This is used in writing log and pid files, so keep that in mind!\r\n# Java property: hadoop.id.str\r\n# export HADOOP_IDENT_STRING=$USER\r\n\r\n# How many seconds to pause after stopping a daemon\r\n\r\n\r\n# Where pid files are stored.  /tmp by default.\r\n# export HADOOP_PID_DIR=/tmp\r\n\r\n# Default log4j setting for interactive commands\r\n# Java property: hadoop.root.logger\r\n# export HADOOP_ROOT_LOGGER=INFO,console\r\n\r\n# Default log4j setting for daemons spawned explicitly by\r\n# --daemon option of hadoop, hdfs, mapred and yarn command.\r\n# Java property: hadoop.root.logger\r\n# export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA\r\n\r\n# Default log level and output location for security-related messages.\r\n# You will almost certainly want to change this on a per-daemon basis via\r\n# the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the\r\n# defaults for the NN and 2NN override this by default.)\r\n# Java property: hadoop.security.logger\r\n# export HADOOP_SECURITY_LOGGER=INFO,NullAppender\r\n\r\n# Default process priority level\r\n# Note that sub-processes will also run at this level!\r\n# export HADOOP_NICENESS=0\r\n\r\n# Default name for the service level authorization file\r\n# Java property: hadoop.policy.file\r\n# export HADOOP_POLICYFILE=\"hadoop-policy.xml\"\r\n\r\n#\r\n# NOTE: this is not used by default!  <-----\r\n# You can define variables right here and then re-use them later on.\r\n# For example, it is common to use the same garbage collection settings\r\n# for all the daemons.  So one could define:\r\n#\r\n# export HADOOP_GC_SETTINGS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps\"\r\n#\r\n# .. and then use it as per the b option under the namenode.\r\n\r\n###\r\n# Secure/privileged execution\r\n###\r\n\r\n#\r\n# Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons\r\n# on privileged ports.  This functionality can be replaced by providing\r\n# custom functions.  See hadoop-functions.sh for more information.\r\n#\r\n\r\n# The jsvc implementation to use. Jsvc is required to run secure datanodes\r\n# that bind to privileged ports to provide authentication of data transfer\r\n# protocol.  Jsvc is not required if SASL is configured for authentication of\r\n# data transfer protocol using non-privileged ports.\r\n# export JSVC_HOME=/usr/bin\r\n\r\n#\r\n# This directory contains pids for secure and privileged processes.\r\n#export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}\r\n\r\n#\r\n# This directory contains the logs for secure and privileged processes.\r\n# Java property: hadoop.log.dir\r\n# export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}\r\n\r\n#\r\n# When running a secure daemon, the default value of HADOOP_IDENT_STRING\r\n# ends up being a bit bogus.  Therefore, by default, the code will\r\n# replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants\r\n# to keep HADOOP_IDENT_STRING untouched, then uncomment this line.\r\n# export HADOOP_SECURE_IDENT_PRESERVE=\"true\"\r\n\r\n###\r\n# NameNode specific parameters\r\n###\r\n\r\n# Default log level and output location for file system related change\r\n# messages. For non-namenode daemons, the Java property must be set in\r\n# the appropriate _OPTS if one wants something other than INFO,NullAppender\r\n# Java property: hdfs.audit.logger\r\n# export HDFS_AUDIT_LOGGER=INFO,NullAppender\r\n\r\n# Specify the JVM options to be used when starting the NameNode.\r\n# These options will be appended to the options specified as HADOOP_OPTS\r\n# and therefore may override any similar flags set in HADOOP_OPTS\r\n#\r\n# a) Set JMX options\r\n# export HDFS_NAMENODE_OPTS=\"-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026\"\r\n#\r\n# b) Set garbage collection logs\r\n# export HDFS_NAMENODE_OPTS=\"${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +\'%Y%m%d%H%M\')\"\r\n#\r\n# c) ... or set them directly\r\n# export HDFS_NAMENODE_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +\'%Y%m%d%H%M\')\"\r\n\r\n# this is the default:\r\n# export HDFS_NAMENODE_OPTS=\"-Dhadoop.security.logger=INFO,RFAS\"\r\n\r\n###\r\n# SecondaryNameNode specific parameters\r\n###\r\n# Specify the JVM options to be used when starting the SecondaryNameNode.\r\n# These options will be appended to the options specified as HADOOP_OPTS\r\n# and therefore may override any similar flags set in HADOOP_OPTS\r\n#\r\n# This is the default:\r\n# export HDFS_SECONDARYNAMENODE_OPTS=\"-Dhadoop.security.logger=INFO,RFAS\"\r\n\r\n###\r\n# DataNode specific parameters\r\n###\r\n# Specify the JVM options to be used when starting the DataNode.\r\n# These options will be appended to the options specified as HADOOP_OPTS\r\n# and therefore may override any similar flags set in HADOOP_OPTS\r\n#\r\n# This is the default:\r\n# export HDFS_DATANODE_OPTS=\"-Dhadoop.security.logger=ERROR,RFAS\"\r\n\r\n# On secure datanodes, user to run the datanode as after dropping privileges.\r\n# This **MUST** be uncommented to enable secure HDFS if using privileged ports\r\n# to provide authentication of data transfer protocol.  This **MUST NOT** be\r\n# defined if SASL is configured for authentication of data transfer protocol\r\n# using non-privileged ports.\r\n# This will replace the hadoop.id.str Java property in secure mode.\r\n# export HDFS_DATANODE_SECURE_USER=hdfs\r\n\r\n# Supplemental options for secure datanodes\r\n# By default, Hadoop uses jsvc which needs to know to launch a\r\n# server jvm.\r\n# export HDFS_DATANODE_SECURE_EXTRA_OPTS=\"-jvm server\"\r\n\r\n###\r\n# NFS3 Gateway specific parameters\r\n###\r\n# Specify the JVM options to be used when starting the NFS3 Gateway.\r\n# These options will be appended to the options specified as HADOOP_OPTS\r\n# and therefore may override any similar flags set in HADOOP_OPTS\r\n#\r\n# export HDFS_NFS3_OPTS=\"\"\r\n\r\n# Specify the JVM options to be used when starting the Hadoop portmapper.\r\n# These options will be appended to the options specified as HADOOP_OPTS\r\n# and therefore may override any similar flags set in HADOOP_OPTS\r\n#\r\n# export HDFS_PORTMAP_OPTS=\"-Xmx512m\"\r\n\r\n# Supplemental options for priviliged gateways\r\n# By default, Hadoop uses jsvc which needs to know to launch a\r\n# server jvm.\r\n# export HDFS_NFS3_SECURE_EXTRA_OPTS=\"-jvm server\"\r\n\r\n# On privileged gateways, user to run the gateway as after dropping privileges\r\n# This will replace the hadoop.id.str Java property in secure mode.\r\n# export HDFS_NFS3_SECURE_USER=nfsserver\r\n\r\n###\r\n# ZKFailoverController specific parameters\r\n###\r\n# Specify the JVM options to be used when starting the ZKFailoverController.\r\n# These options will be appended to the options specified as HADOOP_OPTS\r\n# and therefore may override any similar flags set in HADOOP_OPTS\r\n#\r\n# export HDFS_ZKFC_OPTS=\"\"\r\n\r\n###\r\n# QuorumJournalNode specific parameters\r\n###\r\n# Specify the JVM options to be used when starting the QuorumJournalNode.\r\n# These options will be appended to the options specified as HADOOP_OPTS\r\n# and therefore may override any similar flags set in HADOOP_OPTS\r\n#\r\n# export HDFS_JOURNALNODE_OPTS=\"\"\r\n\r\n###\r\n# HDFS Balancer specific parameters\r\n###\r\n# Specify the JVM options to be used when starting the HDFS Balancer.\r\n# These options will be appended to the options specified as HADOOP_OPTS\r\n# and therefore may override any similar flags set in HADOOP_OPTS\r\n#\r\n# export HDFS_BALANCER_OPTS=\"\"\r\n\r\n###\r\n# HDFS Mover specific parameters\r\n###\r\n# Specify the JVM options to be used when starting the HDFS Mover.\r\n# These options will be appended to the options specified as HADOOP_OPTS\r\n# and therefore may override any similar flags set in HADOOP_OPTS\r\n#\r\n# export HDFS_MOVER_OPTS=\"\"\r\n\r\n###\r\n# Router-based HDFS Federation specific parameters\r\n# Specify the JVM options to be used when starting the RBF Routers.\r\n# These options will be appended to the options specified as HADOOP_OPTS\r\n# and therefore may override any similar flags set in HADOOP_OPTS\r\n#\r\n# export HDFS_DFSROUTER_OPTS=\"\"\r\n###\r\n\r\n###\r\n# Advanced Users Only!\r\n###\r\n\r\n#\r\n# When building Hadoop, one can add the class paths to the commands\r\n# via this special env var:\r\n# export HADOOP_ENABLE_BUILD_PATHS=\"true\"\r\n\r\n#\r\n# To prevent accidents, shell commands be (superficially) locked\r\n# to only allow certain users to execute certain subcommands.\r\n# It uses the format of (command)_(subcommand)_USER.\r\n#\r\n# For example, to limit who can execute the namenode command,\r\n# export HDFS_NAMENODE_USER=hdfs\r\n\r\nTEZ_CONF_DIR=/opt/module/hadoop/etc/hadoop/tez-site.xml\r\nTEZ_JARS=/opt/module/tez\r\nexport HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:${TEZ_CONF_DIR}:${TEZ_JARS}/*:${TEZ_JARS}/lib/*\r\n\r\n```\r\n","timestamp":1649836682854},{"name":"10-hadoop核心配置-core.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/10-hadoop核心配置-core.md","content":"# <font color=#C71585>hadoop核心配置-core</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## hadoop/etc/hadoop/core-site.xml\r\n\r\n```xml\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\r\n<!--\r\n  Licensed under the Apache License, Version 2.0 (the \"License\");\r\n  you may not use this file except in compliance with the License.\r\n  You may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n  Unless required by applicable law or agreed to in writing, software\r\n  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n  See the License for the specific language governing permissions and\r\n  limitations under the License. See accompanying LICENSE file.\r\n-->\r\n\r\n<!-- Put site-specific property overrides in this file. -->\r\n\r\n<configuration>\r\n\r\n  <!-- 开启垃圾箱功能,4320分钟-3天 -->\r\n  <property>\r\n    <name>fs.trash.interval</name>\r\n    <value>4320</value>\r\n  </property>\r\n  <property>\r\n    <name>fs.trash.checkpoint.interval</name>\r\n    <value>4320</value>\r\n  </property>\r\n\r\n  <!-- 指定 NameNode 的地址，指定hdfs的nameservice为ns1,是NameNode的URI。hdfs://主机名:端口/ -->\r\n  <property>\r\n    <name>fs.defaultFS</name>\r\n    <value>hdfs://x6mccluster:8020</value>\r\n  </property>\r\n\r\n  <!-- 读写缓冲大小，默认4096-4K，调到131072-128K -->\r\n  <property>\r\n    <name>io.file.buffer.size</name>\r\n    <value>131072</value>\r\n  </property>\r\n\r\n  <!-- 指定 hadoop 数据的临时目录 -->\r\n  <property>\r\n    <name>hadoop.tmp.dir</name>\r\n    <value>/mnt/hadoop/tmp</value>\r\n  </property>\r\n\r\n  <!-- seqfile io -->\r\n  <property>\r\n    <name>io.seqfile.local.dir</name>\r\n    <value>/mnt/hadoop/io/local</value>\r\n  </property>\r\n\r\n  <!-- 配置 HDFS 网页登录使用的静态用户为 hadoop -->\r\n  <property>\r\n    <name>hadoop.http.staticuser.user</name>\r\n    <value>hadoop</value>\r\n  </property>\r\n\r\n  <!--指定可以在任何IP访问-->\r\n  <!-- 允许通过 httpfs 方式访问 hdfs 的主机名 -->\r\n  <property>\r\n    <name>hadoop.proxyuser.hadoop.hosts</name>\r\n    <value>*</value>\r\n  </property>\r\n\r\n  <!--指定所有用户可以访问-->\r\n  <!--允许通过 httpfs 方式访问 hdfs 的用户组-->\r\n  <property>\r\n    <name>hadoop.proxyuser.hadoop.groups</name>\r\n    <value>*</value>\r\n  </property>\r\n\r\n  <!--MR文件压缩与解压缩lzo 配合hive表使用-->\r\n  <property>\r\n      <name>io.compression.codecs</name>\r\n      <value>\r\n      org.apache.hadoop.io.compress.GzipCodec,\r\n      org.apache.hadoop.io.compress.DefaultCodec,\r\n      org.apache.hadoop.io.compress.BZip2Codec,\r\n      org.apache.hadoop.io.compress.SnappyCodec,\r\n      com.hadoop.compression.lzo.LzoCodec,\r\n      com.hadoop.compression.lzo.LzopCodec\r\n      </value>\r\n  </property>\r\n\r\n  <property>\r\n      <name>io.compression.codec.lzo.class</name>\r\n      <value>com.hadoop.compression.lzo.LzoCodec</value>\r\n  </property>\r\n\r\n  <!-- 指定zookeeper地址 -->\r\n  <property>\r\n    <name>ha.zookeeper.quorum</name>\r\n    <value>hadoop001:2181,hadoop002:2181,hadoop003:2181</value>\r\n  </property>\r\n\r\n</configuration>\r\n\r\n```\r\n","timestamp":1649836682854},{"name":"11-hadoop核心配置-capacity.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/11-hadoop核心配置-capacity.md","content":"# <font color=#C71585>hadoop核心配置-capacity</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## hadoop/etc/hadoop/capacity-site.xml\r\n\r\n```xml\r\n<!--\r\n       Licensed under the Apache License, Version 2.0 (the \"License\");\r\n  you may not use this file except in compliance with the License.\r\n  You may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n  Unless required by applicable law or agreed to in writing, software\r\n  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n  See the License for the specific language governing permissions and\r\n  limitations under the License. See accompanying LICENSE file.\r\n-->\r\n<configuration>\r\n\r\n  <property>\r\n    <name>yarn.scheduler.capacity.maximum-applications</name>\r\n    <value>10000</value>\r\n    <description>\r\n      Maximum number of applications that can be pending and running.\r\n    </description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>\r\n    <value>0.3</value>\r\n    <description>\r\n      Maximum percent of resources in the cluster which can be used to run\r\n      application masters i.e. controls number of concurrent running\r\n      applications.\r\n    </description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.scheduler.capacity.resource-calculator</name>\r\n    <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>\r\n    <description>\r\n      The ResourceCalculator implementation to be used to compare\r\n      Resources in the scheduler.\r\n      The default i.e. DefaultResourceCalculator only uses Memory while\r\n      DominantResourceCalculator uses dominant-resource to compare\r\n      multi-dimensional resources such as Memory, CPU etc.\r\n    </description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.scheduler.capacity.root.queues</name>\r\n    <value>default,hive</value>\r\n    <description>\r\n      The queues at the this level (root is the root queue).\r\n    </description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.scheduler.capacity.root.default.capacity</name>\r\n    <value>90</value>\r\n    <description>Default queue target capacity.</description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>\r\n    <value>1</value>\r\n    <description>\r\n      Default queue user limit a percentage from 0.0 to 1.0.\r\n    </description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>\r\n    <value>100</value>\r\n    <description>\r\n      The maximum capacity of the default queue.\r\n    </description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.scheduler.capacity.root.default.state</name>\r\n    <value>RUNNING</value>\r\n    <description>\r\n      The state of the default queue. State can be one of RUNNING or STOPPED.\r\n    </description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>\r\n    <value>*</value>\r\n    <description>\r\n      The ACL of who can submit jobs to the default queue.\r\n    </description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>\r\n    <value>*</value>\r\n    <description>\r\n      The ACL of who can administer jobs on the default queue.\r\n    </description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.scheduler.capacity.root.default.acl_application_max_priority</name>\r\n    <value>*</value>\r\n    <description>\r\n      The ACL of who can submit applications with configured priority.\r\n      For e.g, [user={name} group={name} max_priority={priority} default_priority={priority}]\r\n    </description>\r\n  </property>\r\n\r\n   <property>\r\n     <name>yarn.scheduler.capacity.root.default.maximum-application-lifetime\r\n     </name>\r\n     <value>-1</value>\r\n     <description>\r\n        Maximum lifetime of an application which is submitted to a queue\r\n        in seconds. Any value less than or equal to zero will be considered as\r\n        disabled.\r\n        This will be a hard time limit for all applications in this\r\n        queue. If positive value is configured then any application submitted\r\n        to this queue will be killed after exceeds the configured lifetime.\r\n        User can also specify lifetime per application basis in\r\n        application submission context. But user lifetime will be\r\n        overridden if it exceeds queue maximum lifetime. It is point-in-time\r\n        configuration.\r\n        Note : Configuring too low value will result in killing application\r\n        sooner. This feature is applicable only for leaf queue.\r\n     </description>\r\n   </property>\r\n\r\n   <property>\r\n     <name>yarn.scheduler.capacity.root.default.default-application-lifetime\r\n     </name>\r\n     <value>-1</value>\r\n     <description>\r\n        Default lifetime of an application which is submitted to a queue\r\n        in seconds. Any value less than or equal to zero will be considered as\r\n        disabled.\r\n        If the user has not submitted application with lifetime value then this\r\n        value will be taken. It is point-in-time configuration.\r\n        Note : Default lifetime can\'t exceed maximum lifetime. This feature is\r\n        applicable only for leaf queue.\r\n     </description>\r\n   </property>\r\n\r\n  <property>\r\n    <name>yarn.scheduler.capacity.node-locality-delay</name>\r\n    <value>40</value>\r\n    <description>\r\n      Number of missed scheduling opportunities after which the CapacityScheduler\r\n      attempts to schedule rack-local containers.\r\n      When setting this parameter, the size of the cluster should be taken into account.\r\n      We use 40 as the default value, which is approximately the number of nodes in one rack.\r\n      Note, if this value is -1, the locality constraint in the container request\r\n      will be ignored, which disables the delay scheduling.\r\n    </description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.scheduler.capacity.rack-locality-additional-delay</name>\r\n    <value>-1</value>\r\n    <description>\r\n      Number of additional missed scheduling opportunities over the node-locality-delay\r\n      ones, after which the CapacityScheduler attempts to schedule off-switch containers,\r\n      instead of rack-local ones.\r\n      Example: with node-locality-delay=40 and rack-locality-delay=20, the scheduler will\r\n      attempt rack-local assignments after 40 missed opportunities, and off-switch assignments\r\n      after 40+20=60 missed opportunities.\r\n      When setting this parameter, the size of the cluster should be taken into account.\r\n      We use -1 as the default value, which disables this feature. In this case, the number\r\n      of missed opportunities for assigning off-switch containers is calculated based on\r\n      the number of containers and unique locations specified in the resource request,\r\n      as well as the size of the cluster.\r\n    </description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.scheduler.capacity.queue-mappings</name>\r\n    <value></value>\r\n    <description>\r\n      A list of mappings that will be used to assign jobs to queues\r\n      The syntax for this list is [u|g]:[name]:[queue_name][,next mapping]*\r\n      Typically this list will be used to map users to queues,\r\n      for example, u:%user:%user maps all users to queues with the same name\r\n      as the user.\r\n    </description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.scheduler.capacity.queue-mappings-override.enable</name>\r\n    <value>false</value>\r\n    <description>\r\n      If a queue mapping is present, will it override the value specified\r\n      by the user? This can be used by administrators to place jobs in queues\r\n      that are different than the one specified by the user.\r\n      The default is false.\r\n    </description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>yarn.scheduler.capacity.per-node-heartbeat.maximum-offswitch-assignments</name>\r\n    <value>1</value>\r\n    <description>\r\n      Controls the number of OFF_SWITCH assignments allowed\r\n      during a node\'s heartbeat. Increasing this value can improve\r\n      scheduling rate for OFF_SWITCH containers. Lower values reduce\r\n      \"clumping\" of applications on particular nodes. The default is 1.\r\n      Legal values are 1-MAX_INT. This config is refreshable.\r\n    </description>\r\n  </property>\r\n\r\n  <!--其他队列的相关配置-->\r\n  <property>\r\n    <name>yarn.scheduler.capacity.application.fail-fast</name>\r\n    <value>false</value>\r\n    <description>\r\n      Whether RM should fail during recovery if previous applications\'\r\n      queue is no longer valid.\r\n    </description>\r\n\r\n    <!--hive队列的相关配置-->\r\n    <property>\r\n      <name>yarn.scheduler.capacity.root.hive.capacity</name>\r\n      <value>10</value>\r\n      <description>hive 队列的容量为 10% </description>\r\n    </property>\r\n\r\n    <property>\r\n      <name>yarn.scheduler.capacity.root.hive.user-limit-factor</name>\r\n      <value>1</value>\r\n      <description>一个用户最多能够获取该队列资源容量的比例，取值 0-1</description>\r\n    </property>\r\n\r\n    <property>\r\n      <name>yarn.scheduler.capacity.root.hive.maximum-capacity</name>\r\n      <value>100</value>\r\n      <description>hive 队列的最大容量(自己队列资源不够，可以使用其他队列资源上限) </description>\r\n    </property>\r\n\r\n    <property>\r\n      <name>yarn.scheduler.capacity.root.hive.state</name>\r\n      <value>RUNNING</value>\r\n      <description>开启 hive 队列运行，不设置队列不能使用</description>\r\n    </property>\r\n\r\n    <property>\r\n      <name>yarn.scheduler.capacity.root.hive.acl_submit_applications</name>\r\n      <value>*</value>\r\n      <description>访问控制，控制谁可以将任务提交到该队列,*表示任何人 </description>\r\n    </property>\r\n\r\n    <property>\r\n      <name>yarn.scheduler.capacity.root.hive.acl_administer_queue</name>\r\n      <value>*</value>\r\n      <description>访问控制，控制谁可以管理(包括提交和取消)该队列的任务，*表示任何人</description>\r\n    </property>\r\n\r\n    <property>\r\n      <name>yarn.scheduler.capacity.root.hive.acl_application_max_priority</name>\r\n      <value>*</value>\r\n      <description>指定哪个用户可以提交配置任务优先级</description>\r\n    </property>\r\n\r\n    <property>\r\n      <name>yarn.scheduler.capacity.root.hive.maximum-application-lifetime</name>\r\n      <value>-1</value>\r\n      <description>hive 队列中任务的最大生命时长，以秒为单位。任何小于或等于零的值将被视为禁用。</description>\r\n    </property>\r\n\r\n    <property>\r\n      <name>yarn.scheduler.capacity.root.hive.default-application-lifetime</name>\r\n      <value>-1</value>\r\n      <description>hive 队列中任务的默认生命时长，以秒为单位。任何小于或等于零的值将被视为禁用。 </description>\r\n    </property>\r\n  </property>\r\n\r\n</configuration>\r\n\r\n```\r\n","timestamp":1649836682854},{"name":"12-hadoop核心配置-works.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/12-hadoop核心配置-works.md","content":"# <font color=#C71585>hadoop核心配置-work</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## hadoop/etc/hadoop/workers\r\n\r\n```xml\r\nhadoop001\r\nhadoop002\r\nhadoop003\r\n```\r\n","timestamp":1649836682854},{"name":"13-hive核心配置-spark.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/13-hive核心配置-spark.md","content":"# <font color=#C71585>hive核心配置-spark</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## /opt/module/hive/conf/spark-defaults.conf\r\n\r\n```xml\r\nspark.master yarn\r\nspark.eventLog.enabled true\r\nspark.eventLog.dir hdfs://test-x6mccluster:8020/spark/spark-log\r\nspark.executor.memory 4g\r\nspark.driver.memory 1g\r\n\r\n```\r\n","timestamp":1649836682854},{"name":"14-hive核心配置-tez.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/14-hive核心配置-tez.md","content":"# <font color=#C71585>hive核心配置-tez</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## /opt/module/hive/conf/tez-site.xml\r\n\r\n```xml\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\r\n<configuration>\r\n  <property>\r\n    <name>tez.lib.uris</name>\r\n    <value>${fs.defaultFS}/tez/tez.tar.gz</value>\r\n  </property>\r\n  <property>\r\n     <name>tez.use.cluster.hadoop-libs</name>\r\n     <value>true</value>\r\n  </property>\r\n  <property>\r\n     <name>tez.history.logging.service.class</name>\r\n     <value>org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService</value>\r\n  </property>\r\n</configuration>\r\n\r\n```\r\n","timestamp":1649836682854},{"name":"15-hive核心配置-hive.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/15-hive核心配置-hive.md","content":"# <font color=#C71585>hive核心配置-hive</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## /opt/module/hive/conf/hive-site.xml\r\n\r\n```xml\r\n<?xml version=\"1.0\"?>\r\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\r\n<configuration>\r\n<!-- jdbc 连接的 URL -->\r\n   <property>\r\n       <name>javax.jdo.option.ConnectionURL</name>\r\n       <value>jdbc:mysql://域名:端口/hive?useSSL=false&amp;createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8</value>\r\n   </property>\r\n<!-- jdbc 连接的 Driver-->\r\n   <property>\r\n       <name>javax.jdo.option.ConnectionDriverName</name>\r\n       <value>com.mysql.jdbc.Driver</value>\r\n   </property>\r\n<!-- jdbc 连接的 username-->\r\n   <property>\r\n       <name>javax.jdo.option.ConnectionUserName</name>\r\n       <value>用户名</value>\r\n   </property>\r\n<!-- jdbc 连接的 password -->\r\n   <property>\r\n       <name>javax.jdo.option.ConnectionPassword</name>\r\n       <value>密码</value>\r\n   </property>\r\n<!-- Hive 元数据存储版本的验证 -->\r\n   <property>\r\n      <name>hive.metastore.schema.verification</name>\r\n      <value>false</value>\r\n    </property>\r\n<!--元数据存储授权-->\r\n  <property>\r\n      <name>hive.metastore.event.db.notification.api.auth</name>\r\n      <value>false</value>\r\n  </property>\r\n<!-- Hive 默认在 HDFS 的工作目录 -->\r\n  <property>\r\n      <name>hive.metastore.warehouse.dir</name>\r\n      <value>/user/hive/warehouse</value>\r\n  </property>\r\n  <!-- 指定存储元数据要连接的地址 -->\r\n  <property>\r\n    <name>hive.metastore.uris</name>\r\n    <value>thrift://hadoop001:9083,thrift://hadoop002:9083</value>\r\n  </property>\r\n\r\n  <!--hiveserver2的HA-->\r\n  <property>\r\n    <name>hive.zookeeper.quorum</name>\r\n    <value>hadoop001,hadoop002,hadoop003</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>hive.zookeeper.namespace</name>\r\n    <value>hiveserver2</value>\r\n    <description>The parent node under which all ZooKeeper nodes are created.</description>\r\n  </property>\r\n\r\n  <!--权限配置-->\r\n<!--\r\n  <property>\r\n    <name>hive.semantic.analyzer.hook</name>\r\n    <value>com.x6mc.HiveSuperAuthority.SuperAuthority</value>\r\n    <description/>\r\n  </property>\r\n\r\n  <property>\r\n    <name>hive.security.authorization.enabled</name>\r\n    <value>true</value>\r\n    <description>enable or disable the Hive client authorization</description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>hive.security.authorization.manager</name>\r\n    <value>org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider</value>\r\n    <description>\r\n      The Hive client authorization manager class name. The user defined authorization class should implement\r\n      interface org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider.\r\n    </description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>hive.security.authorization.createtable.user.grants</name>\r\n    <value>hadoop:ALL</value>\r\n    <description>\r\n      the privileges automatically granted to some users whenever a table gets created.\r\n      An example like \"userX,userY:select;userZ:create\" will grant select privilege to userX and userY,\r\n      and grant create privilege to userZ whenever a new table created.\r\n    </description>\r\n  </property>\r\n-->\r\n  <property>\r\n    <name>hive.security.authorization.createtable.owner.grants</name>\r\n    <value>ALL</value>\r\n    <description>\r\n      The privileges automatically granted to the owner whenever a table gets created.\r\n      An example like \"select,drop\" will grant select and drop privilege to the owner\r\n      of the table. Note that the default gives the creator of a table no access to the\r\n      table (but see HIVE-8067).\r\n    </description>\r\n  </property>\r\n\r\n  <!-- log 信息 -->\r\n  <property>\r\n    <name>hive.server2.logging.operation.log.location</name>\r\n    <value>/tmp/hive/operation_logs</value>\r\n    <description>Top level directory where operation logs are stored if logging functionality is enabled</description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>hive.downloaded.resources.dir</name>\r\n    <value>/tmp/hive/resources</value>\r\n    <description>Temporary local directory for added resources in the remote file system.</description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>hive.querylog.location</name>\r\n    <value>/tmp/hive/querylog</value>\r\n    <description>Location of Hive run time structured log file</description>\r\n  </property>\r\n\r\n  <!-- hiveserver2的高可用参数，开启此参数可以提高hiveserver2的启动速度 -->\r\n  <property>\r\n    <name>hive.server2.active.passive.ha.enable</name>\r\n    <value>true</value>\r\n  </property>\r\n\r\n  <!--有多台hiveserver2时开启，否则开启会报错-->\r\n  <property>\r\n    <name>hive.server2.support.dynamic.service.discovery</name>\r\n    <value>true</value>\r\n    <description>Whether HiveServer2 supports dynamic service discovery for its clients. To support this, each instance of HiveServer2 currently uses ZooKeeper to register itself, when it is brought up. JDBC/ODBC clients should use the ZooKeeper ensemble: hive.zookeeper.quorum in their connection string.</description>\r\n  </property>\r\n\r\n\r\n  <!--身份验证模式，默认为NONE。可选项是NONE.NOSASL,KERBEROS,KERBEROS,PAM,CUSTOM\r\n  <property>\r\n    <name>hive.server2.authentication</name>\r\n    <value>LDAP</value>\r\n  </property>\r\n  <property>\r\n    <name>hive.server2.authentication.ldap.url</name>\r\n    <value>ldap://hadoop002</value>\r\n  </property>\r\n  <property>\r\n    <name>hive.server2.authentication.ldap.baseDN</name>\r\n    <value>ou=hadoop,o=x6mc@hive</value>\r\n  </property>\r\n-->\r\n\r\n  <!-- 指定 hiveserver2 连接的 host -->\r\n  <property>\r\n    <name>hive.server2.thrift.bind.host</name>\r\n    <value>hadoop002</value>\r\n  </property>\r\n  <!-- 指定 hiveserver2 连接的端口号 -->\r\n  <property>\r\n   <name>hive.server2.thrift.port</name>\r\n   <value>10000</value>\r\n  </property>\r\n\r\n  <!-- 指定 hiveserver2 连接的用户 -->\r\n  <property>\r\n    <name>hive.server2.thrift.client.user</name>\r\n    <value>hadoop</value>\r\n  </property>\r\n\r\n  <!-- 指定 hiveserver2 连接的密码 -->\r\n  <property>\r\n    <name>hive.server2.thrift.client.password</name>\r\n    <value>x6mc@hive</value>\r\n  </property>\r\n\r\n  <property>\r\n   <name>hive.cli.print.header</name>\r\n   <value>true</value>\r\n  </property>\r\n  <property>\r\n     <name>hive.cli.print.current.db</name>\r\n     <value>true</value>\r\n  </property>\r\n  <property>\r\n     <name>hive.metastore.schema.verification</name>\r\n     <value>false</value>\r\n  </property>\r\n\r\n  <!--Spark 依赖位置(注意:端口号 8020 必须和 namenode 的端口号一致)-->\r\n  <property>\r\n     <name>spark.yarn.jars</name>\r\n     <value>hdfs://x6mccluster:8020/spark/spark-jars/*</value>\r\n  </property>\r\n\r\n  <!--Hive 执行引擎-->\r\n  <property>\r\n     <name>hive.execution.engine</name>\r\n     <value>mr</value>\r\n  </property>\r\n\r\n  <property>\r\n     <name>hive.server2.webui.host</name>\r\n     <value>hadoop002</value>\r\n  </property>\r\n  <property>\r\n     <name>hive.server2.webui.port</name>\r\n     <value>10002</value>\r\n  </property>\r\n\r\n  <!-- hive on spark-->\r\n  <property>\r\n    <name>spark.home</name>\r\n    <value>/share/opt/module/spark</value>\r\n  </property>\r\n  <property>\r\n    <name>spark.master</name>\r\n    <value>yarn</value>\r\n  </property>\r\n  <property>\r\n    <name>spark.deploy.mode</name>\r\n    <value>cluster</value>\r\n  </property>\r\n  <property>\r\n    <name>hive.enable.spark.execution.engine</name>\r\n    <value>true</value>\r\n  </property>\r\n  <property>\r\n    <name>spark.enentLog.enabled</name>\r\n    <value>true</value>\r\n  </property>\r\n  <property>\r\n    <name>spark.enentLog.dir</name>\r\n    <value>hdfs://x6mccluster:8020/spark/spark-log</value>\r\n  </property>\r\n  <property>\r\n    <name>spark.serializer</name>\r\n    <value>org.apache.spark.serializer.KryoSerializer</value>\r\n  </property>\r\n  <property>\r\n    <name>spark.executor.memeory</name>\r\n    <value>4g</value>\r\n  </property>\r\n  <property>\r\n    <name>spark.driver.memeory</name>\r\n    <value>1g</value>\r\n  </property>\r\n  <property>\r\n    <name>spark.executor.extraJavaOptions</name>\r\n    <value>-XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"</value>\r\n  </property>\r\n\r\n  <property>\r\n    <name>hive.spark.client.rpc.threads</name>\r\n    <value>60</value>\r\n    <description>Maximum number of threads for remote Spark driver\'s RPC event loop.</description>\r\n  </property>\r\n\r\n  <property>\r\n     <name>hive.spark.client.connect.timeout</name>\r\n     <value>100000ms</value>\r\n  </property>\r\n\r\n  <!--hive引入包,此配置项对于hive server有效,不会作用到hive shell-->\r\n  <property>\r\n    <name>hive.aux.jars.path</name>\r\n    <value>file:///share/opt/module/lib_all/es-lib/elasticsearch-hadoop-hive-7.6.1.jar,file:///share/opt/module/lib_all/es-lib/commons-httpclient-3.1.jar</value>\r\n  </property>\r\n\r\n  <!--hive参数调优-->\r\n  <property>\r\n    <name>hive.merge.mapredfiles</name>\r\n    <value>true</value>\r\n    <description>Merge small files at the end of a map-reduce job</description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>hive.merge.sparkfiles</name>\r\n    <value>true</value>\r\n    <description>Merge small files at the end of a Spark DAG Transformation</description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>hive.merge.size.per.task</name>\r\n    <value>128000000</value>\r\n    <description>Size of merged files at the end of the job</description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>hive.exec.dynamic.partition</name>\r\n    <value>true</value>\r\n    <description>Whether or not to allow dynamic partitions in DML/DDL.</description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>hive.exec.dynamic.partition.mode</name>\r\n    <value>nostrict</value>\r\n    <description>\r\n      In strict mode, the user must specify at least one static partition\r\n      in case the user accidentally overwrites all partitions.\r\n      In nonstrict mode all partitions are allowed to be dynamic.\r\n    </description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>hive.exec.max.dynamic.partitions</name>\r\n    <value>100000</value>\r\n    <description>Maximum number of dynamic partitions allowed to be created in total.</description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>hive.exec.max.dynamic.partitions.pernode</name>\r\n    <value>10000</value>\r\n    <description>Maximum number of dynamic partitions allowed to be created in each mapper/reducer node.</description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>hive.exec.max.created.files</name>\r\n    <value>150000</value>\r\n    <description>Maximum number of HDFS files created by all mappers/reducers in a MapReduce job.</description>\r\n  </property>\r\n\r\n</configuration>\r\n\r\n```\r\n","timestamp":1649836682854},{"name":"16-hive核心配置-log4j2.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/16-hive核心配置-log4j2.md","content":"# <font color=#C71585>hive核心配置-log4j2</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## /opt/module/hive/conf/hive-log4j2.properties\r\n\r\n```xml\r\n# Licensed to the Apache Software Foundation (ASF) under one\r\n# or more contributor license agreements.  See the NOTICE file\r\n# distributed with this work for additional information\r\n# regarding copyright ownership.  The ASF licenses this file\r\n# to you under the Apache License, Version 2.0 (the\r\n# \"License\"); you may not use this file except in compliance\r\n# with the License.  You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\nstatus = INFO\r\nname = HiveLog4j2\r\npackages = org.apache.hadoop.hive.ql.log\r\n\r\n# list of properties\r\nproperty.hive.log.level = INFO\r\nproperty.hive.root.logger = DRFA\r\nproperty.hive.log.dir = /opt/module/hive/logs\r\nproperty.hive.log.file = hive.log\r\nproperty.hive.perflogger.log.level = INFO\r\n\r\n# list of all appenders\r\nappenders = console, DRFA\r\n\r\n# console appender\r\nappender.console.type = Console\r\nappender.console.name = console\r\nappender.console.target = SYSTEM_ERR\r\nappender.console.layout.type = PatternLayout\r\nappender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n\r\n\r\n# daily rolling file appender\r\nappender.DRFA.type = RollingRandomAccessFile\r\nappender.DRFA.name = DRFA\r\nappender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}\r\n# Use %pid in the filePattern to append <process-id>@<host-name> to the filename if you want separate log files for different CLI session\r\nappender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}\r\nappender.DRFA.layout.type = PatternLayout\r\nappender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n\r\nappender.DRFA.policies.type = Policies\r\nappender.DRFA.policies.time.type = TimeBasedTriggeringPolicy\r\nappender.DRFA.policies.time.interval = 1\r\nappender.DRFA.policies.time.modulate = true\r\nappender.DRFA.strategy.type = DefaultRolloverStrategy\r\nappender.DRFA.strategy.max = 30\r\n\r\n# list of all loggers\r\nloggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger, AmazonAws, ApacheHttp\r\n\r\nlogger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn\r\nlogger.NIOServerCnxn.level = WARN\r\n\r\nlogger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO\r\nlogger.ClientCnxnSocketNIO.level = WARN\r\n\r\nlogger.DataNucleus.name = DataNucleus\r\nlogger.DataNucleus.level = ERROR\r\n\r\nlogger.Datastore.name = Datastore\r\nlogger.Datastore.level = ERROR\r\n\r\nlogger.JPOX.name = JPOX\r\nlogger.JPOX.level = ERROR\r\n\r\nlogger.AmazonAws.name=com.amazonaws\r\nlogger.AmazonAws.level = INFO\r\n\r\nlogger.ApacheHttp.name=org.apache.http\r\nlogger.ApacheHttp.level = INFO\r\n\r\nlogger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger\r\nlogger.PerfLogger.level = ${sys:hive.perflogger.log.level}\r\n\r\n# root logger\r\nrootLogger.level = ${sys:hive.log.level}\r\nrootLogger.appenderRefs = root\r\nrootLogger.appenderRef.root.ref = ${sys:hive.root.logger}\r\n\r\n```\r\n","timestamp":1649836682854},{"name":"17-scala-2.12.15安装.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/17-scala-2.12.15安装.md","content":"# <font color=#C71585>scala-2.12.15安装</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## 下载安装包\r\n\r\nhttps://www.scala-lang.org/download/2.12.15.html\r\n\r\n## 解压安装包\r\n```\r\n[hadoop@hadoop001 software]$ tar -xvf /opt/software/scala-2.12.15.tgz -C /opt/module/\r\n```\r\n## 配置环境变量\r\n```\r\n[hadoop@hadoop001 module]$ sudo vim /etc/profile.d/my_env.sh\r\n\r\n#scala\r\nexport SCALA_HOME=/opt/module/scala-2.12.15\r\nexport PATH=$PATH:$SCALA_HOME/bin\r\n```\r\n```\r\n[hadoop@hadoop001 module]$ source /etc/profile\r\n```\r\n##\t验证scala版本<更改需断开客户端重连>\r\n```\r\n[hadoop@hadoop001 scala-2.12.15]$ scala -version\r\n```\r\n","timestamp":1649836682854},{"name":"18-Kafka-2.12-3.0.0安装.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/18-Kafka-2.12-3.0.0安装.md","content":"# <font color=#C71585>Kafka-2.12-3.0.0安装</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## 下载安装包\r\n\r\nhttps://kafka.apache.org/downloads\r\n\r\n| 主机名     | broker.id     | work |\r\n| :-------------: | :-------------: | :-------------: |\r\n| hadoop001      | 1      | ✔ |\r\n| hadoop002      | 2      | ✔ |\r\n| hadoop003      | 3      | ✔ |\r\n\r\n## 解压安装包\r\n```\r\n[hadoop@hadoop001 software]$ tar -xvf /opt/software/kafka_2.12-3.0.0.tgz -C /opt/module/\r\n```\r\n## 修改配置文件-server.properties\r\n```\r\nbroker.id=1\r\nlisteners=PLAINTEXT://hadoop001:9092\r\nadvertised.listeners=PLAINTEXT://hadoop001:9092\r\nlog.dirs=/opt/module/kafka_2.12-3.0.0/logs\r\nnum.partitions=2\r\nzookeeper.connect=hadoop001:2181,hadoop002:2181,hadoop003:2181\r\n```\r\n1.\t其中broker.id每个节点配置一个编号，保证唯一  \r\n2.\tlisteners就是主要用来定义Kafka Broker的Listener的配置项  \r\n3.\tadvertised.listeners参数的作用就是将Broker的Listener信息发布到Zookeeper  \r\n4.\tlog.dirs为日志路径  \r\n5.\tzookeeper为zookeeper集群的地址\r\n## 修改配置文件-consumer.properties\r\n```\r\nbootstrap.servers=hadoop001:9092,hadoop002:9092,hadoop003:9092\r\n```\r\n## 修改配置文件-producer.properties\r\n```\r\nbootstrap.servers=hadoop001:9092,hadoop002:9092,hadoop003:9092\r\n```\r\n## 增加启动端口监听\r\n```\r\n[hadoop@hadoop003 bin]$ vim kafka-server-start.sh\r\n```\r\n```js\r\nif [ \"x$KAFKA_HEAP_OPTS\" = \"x\" ]; then\r\n    export KAFKA_HEAP_OPTS=\"-Xmx1G -Xms1G\"\r\n    export JMX_PORT=\"9999\"\r\nfi\r\n```\r\n## 分发到各个节点并修改broker.id、listeners、advertised.listeners为对应值\r\n```\r\n[hadoop@hadoop001 module]$ xsync kafka_2.12-3.0.0\r\n```\r\n## 启动kafka服务\r\n```\r\n[hadoop@hadoop001 bin]$ ./kafka-server-start.sh -daemon ../config/server.properties\r\n```\r\n## 测试\r\n```\r\n-- 创建topic\r\n./kafka-topics.sh --create --partitions 2 --replication-factor 1 --topic quickstart-events --bootstrap-server hadoop001:9092\r\n-- 查看topic信息\r\n./kafka-topics.sh --describe --topic quickstart-events --bootstrap-server hadoop001:9092\r\n-- 生产者\r\n./kafka-console-producer.sh --topic quickstart-events --bootstrap-server hadoop001:9092\r\n-- 消费者\r\n./kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server hadoop001:9092\r\n```\r\n","timestamp":1649836682854},{"name":"19-Kafka Eagle2.0.9安装.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/19-Kafka Eagle2.0.9安装.md","content":"# <font color=#C71585>Kafka Eagle2.0.9安装</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## 下载安装包\r\n\r\nhttps://github.com/smartloli/kafka-eagle-bin/tags\r\n\r\n| 主机名     | Master    | slave |\r\n| :-------------: | :-------------: | :-------------: |\r\n| hadoop001      | ✖      | ✔ |\r\n| hadoop002      | ✖      | ✔ |\r\n| hadoop003      | ✔      | ✖ |\r\n\r\n## 2)\t官方doc文档\r\n\r\nhttp://www.kafka-eagle.org/articles/docs/introduce/getting-started.html\r\n\r\n## 解压安装包\r\n```\r\n[hadoop@hadoop003 software]$ tar -xzvf /opt/software/kafka-eagle-bin-2.0.9.tar.gz -C /opt/module/\r\n[hadoop@hadoop003 software]$ cd ../module/kafka-eagle-bin-2.0.9/\r\n[hadoop@hadoop003 kafka-eagle-bin-2.0.9]$ tar -xzvf /opt/module/kafka-eagle-bin-2.0.9/efak-web-2.0.9-bin.tar.gz -C /opt/module/kafka-eagle-bin-2.0.9/\r\n```\r\n## 配置环境变量\r\n```\r\n[hadoop@hadoop003 module]$ sudo vim /etc/profile.d/my_env.sh\r\n```\r\n```\r\n#kafka-eagle\r\nexport KE_HOME=/opt/module/kafka-eagle-bin-2.0.9/efak-web-2.0.9\r\nexport PATH=$PATH:$KE_HOME/bin\r\n```\r\n```\r\n[hadoop@ hadoop003 module]$ source /etc/profile\r\n```\r\n## 修改配置文件\r\n**system-config.properties**\r\n```\r\n######################################\r\n# multi zookeeper & kafka cluster list\r\n# Settings prefixed with \'kafka.eagle.\' will be deprecated, use \'efak.\' instead\r\n######################################\r\n# efak.zk.cluster.alias=cluster1,cluster2\r\nefak.zk.cluster.alias=cluster1\r\ncluster1.zk.list=hadoop001:2181,hadoop002:2181,hadoop003:2181\r\n# cluster2.zk.list=xdn10:2181,xdn11:2181,xdn12:2181\r\n\r\n######################################\r\n# zookeeper enable acl\r\n######################################\r\ncluster1.zk.acl.enable=false\r\ncluster1.zk.acl.schema=digest\r\ncluster1.zk.acl.username=test\r\ncluster1.zk.acl.password=test123\r\n\r\n######################################\r\n# broker size online list\r\n######################################\r\ncluster1.efak.broker.size=20\r\n\r\n######################################\r\n# zk client thread limit\r\n######################################\r\nkafka.zk.limit.size=32\r\n\r\n######################################\r\n# EFAK webui port\r\n######################################\r\nefak.webui.port=8048\r\n\r\n######################################\r\n# EFAK enable distributed\r\n######################################\r\nefak.distributed.enable=true\r\nefak.cluster.mode.status=master\r\nefak.worknode.master.host=hadoop003\r\nefak.worknode.port=8085\r\n\r\n######################################\r\n# kafka jmx acl and ssl authenticate\r\n######################################\r\ncluster1.efak.jmx.acl=false\r\ncluster1.efak.jmx.user=keadmin\r\ncluster1.efak.jmx.password=keadmin123\r\ncluster1.efak.jmx.ssl=false\r\ncluster1.efak.jmx.truststore.location=/data/ssl/certificates/kafka.truststore\r\ncluster1.efak.jmx.truststore.password=ke123456\r\n\r\n######################################\r\n# kafka offset storage\r\n######################################\r\ncluster1.efak.offset.storage=kafka\r\ncluster2.efak.offset.storage=zk\r\n\r\n######################################\r\n# kafka jmx uri\r\n######################################\r\ncluster1.efak.jmx.uri=service:jmx:rmi:///jndi/rmi://%s/jmxrmi\r\n# cluster1.efak.jmx.uri=service:jmx:rmi:///jndi/rmi://hadoop003:9999/jmxrmi\r\n\r\n######################################\r\n# kafka metrics, 15 days by default\r\n######################################\r\nefak.metrics.charts=true\r\nefak.metrics.retain=60\r\n\r\n######################################\r\n# kafka sql topic records max\r\n######################################\r\nefak.sql.topic.records.max=5000\r\nefak.sql.topic.preview.records.max=10\r\n\r\n######################################\r\n# delete kafka topic token\r\n######################################\r\nefak.topic.token=keadmin\r\n\r\n######################################\r\n# kafka sasl authenticate\r\n######################################\r\ncluster1.efak.sasl.enable=false\r\ncluster1.efak.sasl.protocol=SASL_PLAINTEXT\r\ncluster1.efak.sasl.mechanism=SCRAM-SHA-256\r\ncluster1.efak.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"kafka\" password=\"kafka-eagle\";\r\ncluster1.efak.sasl.client.id=\r\ncluster1.efak.blacklist.topics=\r\ncluster1.efak.sasl.cgroup.enable=false\r\ncluster1.efak.sasl.cgroup.topics=\r\ncluster2.efak.sasl.enable=false\r\ncluster2.efak.sasl.protocol=SASL_PLAINTEXT\r\ncluster2.efak.sasl.mechanism=PLAIN\r\ncluster2.efak.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"kafka\" password=\"kafka-eagle\";\r\ncluster2.efak.sasl.client.id=\r\ncluster2.efak.blacklist.topics=\r\ncluster2.efak.sasl.cgroup.enable=false\r\ncluster2.efak.sasl.cgroup.topics=\r\n\r\n######################################\r\n# kafka ssl authenticate\r\n######################################\r\ncluster3.efak.ssl.enable=false\r\ncluster3.efak.ssl.protocol=SSL\r\ncluster3.efak.ssl.truststore.location=\r\ncluster3.efak.ssl.truststore.password=\r\ncluster3.efak.ssl.keystore.location=\r\ncluster3.efak.ssl.keystore.password=\r\ncluster3.efak.ssl.key.password=\r\ncluster3.efak.ssl.endpoint.identification.algorithm=https\r\ncluster3.efak.blacklist.topics=\r\ncluster3.efak.ssl.cgroup.enable=false\r\ncluster3.efak.ssl.cgroup.topics=\r\n\r\n######################################\r\n# kafka sqlite jdbc driver address\r\n######################################\r\n#efak.driver=org.sqlite.JDBC\r\n#efak.url=jdbc:sqlite:/hadoop/kafka-eagle/db/ke.db\r\n#efak.username=root\r\n#efak.password=www.kafka-eagle.org\r\n\r\n######################################\r\n# kafka mysql jdbc driver address\r\n######################################\r\nefak.driver=com.mysql.cj.jdbc.Driver\r\nefak.url=jdbc:mysql://端口:port/efka?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull\r\nefak.username=用户名\r\nefak.password=密码\r\n```\r\n**works**\r\n```\r\nhadoop001\r\nhadoop002\r\n```\r\n## 分发安装包并修改slave参数\r\n```\r\n[hadoop@ hadoop002 module]$ xsync kafka-eagle-bin-2.0.9\r\n[hadoop@ hadoop002 module]$ vim kafka-eagle-bin-2.0.9/efak-web-2.0.9/conf/system-config.properties\r\nefak.cluster.mode.status=slave\r\n```\r\n## 启动集群\r\n```\r\n[hadoop@hadoop003 bin]$ ./ke.sh cluster start\r\n```\r\n## 查看UI页面\r\nhttp://hadoop003:8048/cluster/info\r\n","timestamp":1649836682854},{"name":"20-Flink-1.13.5-2.12 on yarn安装.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/20-Flink-1.13.5-2.12 on yarn安装.md","content":"# <font color=#C71585>Flink-1.13.5-2.12 on yarn安装</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## 下载安装包\r\n\r\nhttps://flink.apache.org/zh/downloads.html\r\n\r\n| 主机名     | Master-8081    | slave |\r\n| :-------------: | :-------------: | :-------------: |\r\n| hadoop001      | ✖      | ✔ |\r\n| hadoop002      | ✔      | ✔ |\r\n| hadoop003      | ✔      | ✔ |\r\n\r\n## 解压安装包\r\n```\r\n[hadoop@hadoop001 software]$ tar -xvf /opt/software/flink-1.13.5-bin-scala_2.12.tgz -C /opt/module/\r\n```\r\n## 3台机器均增加yarn的HADOOP_CLASSPATH配置\r\n```\r\n[hadoop@hadoop003 module]$ sudo vim /etc/profile.d/my_env.sh\r\n```\r\n```\r\n#HADOOP_HOME\r\nexport HADOOP_CLASSPATH=`hadoop classpath`\r\n```\r\n```\r\n[hadoop@ hadoop003 module]$ source /etc/profile\r\n[hadoop@ hadoop003 module]$ echo $HADOOP_CLASSPATH\r\n```\r\n## 配置环境变量\r\n```\r\n[hadoop@hadoop003 module]$ sudo vim /etc/profile.d/my_env.sh\r\n```\r\n```\r\n#flink\r\nexport FLINK_HOME=/opt/module/flink-1.13.5\r\nexport PATH=$PATH:$KE_HOME/bin\r\n```\r\n```\r\n[hadoop@ hadoop003 module]$ source /etc/profile\r\n```\r\n## hdfs新建文件夹\r\n```\r\nflink/ha/\r\nflink/checkpoints/\r\nflink/savepoints/\r\n```\r\n## 配置文件修改\r\n**zoo.cfg**\r\n```\r\n################################################################################\r\n#  Licensed to the Apache Software Foundation (ASF) under one\r\n#  or more contributor license agreements.  See the NOTICE file\r\n#  distributed with this work for additional information\r\n#  regarding copyright ownership.  The ASF licenses this file\r\n#  to you under the Apache License, Version 2.0 (the\r\n#  \"License\"); you may not use this file except in compliance\r\n#  with the License.  You may obtain a copy of the License at\r\n#\r\n#      http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n#  Unless required by applicable law or agreed to in writing, software\r\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n#  See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n################################################################################\r\n\r\n# The number of milliseconds of each tick\r\ntickTime=2000\r\n\r\n# The number of ticks that the initial  synchronization phase can take\r\ninitLimit=10\r\n\r\n# The number of ticks that can pass between  sending a request and getting an acknowledgement\r\nsyncLimit=5\r\n\r\n# The directory where the snapshot is stored.\r\n# dataDir=/tmp/zookeeper\r\n\r\n# The port at which the clients will connect\r\nclientPort=2181\r\n\r\n# ZooKeeper quorum peers\r\nserver.1=hadoop001:2888:3888\r\nserver.2=hadoop002:2888:3888\r\nserver.3=hadoop003:2888:3888\r\n```\r\n**flink-conf**\r\n```\r\n################################################################################\r\n#  Licensed to the Apache Software Foundation (ASF) under one\r\n#  or more contributor license agreements.  See the NOTICE file\r\n#  distributed with this work for additional information\r\n#  regarding copyright ownership.  The ASF licenses this file\r\n#  to you under the Apache License, Version 2.0 (the\r\n#  \"License\"); you may not use this file except in compliance\r\n#  with the License.  You may obtain a copy of the License at\r\n#\r\n#      http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n#  Unless required by applicable law or agreed to in writing, software\r\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n#  See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n################################################################################\r\n\r\n\r\n#==============================================================================\r\n# Common\r\n#==============================================================================\r\n\r\n# The external address of the host on which the JobManager runs and can be\r\n# reached by the TaskManagers and any clients which want to connect. This setting\r\n# is only used in Standalone mode and may be overwritten on the JobManager side\r\n# by specifying the --host <hostname> parameter of the bin/jobmanager.sh executable.\r\n# In high availability mode, if you use the bin/start-cluster.sh script and setup\r\n# the conf/masters file, this will be taken care of automatically. Yarn/Mesos\r\n# automatically configure the host name based on the hostname of the node where the\r\n# JobManager runs.\r\n\r\njobmanager.rpc.address: hadoop003\r\n\r\n# The RPC port where the JobManager is reachable.\r\n\r\njobmanager.rpc.port: 6123\r\n\r\n\r\n# The total process memory size for the JobManager.\r\n#\r\n# Note this accounts for all memory usage within the JobManager process, including JVM metaspace and other overhead.\r\n\r\njobmanager.memory.process.size: 1600m\r\n\r\n\r\n# The total process memory size for the TaskManager.\r\n#\r\n# Note this accounts for all memory usage within the TaskManager process, including JVM metaspace and other overhead.\r\n\r\ntaskmanager.memory.process.size: 1728m\r\n\r\n# To exclude JVM metaspace and overhead, please, use total Flink memory size instead of \'taskmanager.memory.process.size\'.\r\n# It is not recommended to set both \'taskmanager.memory.process.size\' and Flink memory.\r\n#\r\n# taskmanager.memory.flink.size: 1280m\r\n\r\n# The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.\r\n\r\ntaskmanager.numberOfTaskSlots: 4\r\n\r\n# The parallelism used for programs that did not specify and other parallelism.\r\n\r\nparallelism.default: 2\r\n\r\n# The default file system scheme and authority.\r\n#\r\n# By default file paths without scheme are interpreted relative to the local\r\n# root file system \'file:///\'. Use this to override the default and interpret\r\n# relative paths relative to a different file system,\r\n# for example \'hdfs://mynamenode:12345\'\r\n#\r\n# fs.default-scheme\r\n\r\n#==============================================================================\r\n# High Availability\r\n#==============================================================================\r\n\r\n# The high-availability mode. Possible options are \'NONE\' or \'zookeeper\'.\r\n#\r\n# high-availability: zookeeper\r\nhigh-availability: zookeeper\r\n\r\n# The path where metadata for master recovery is persisted. While ZooKeeper stores\r\n# the small ground truth for checkpoint and leader election, this location stores\r\n# the larger objects, like persisted dataflow graphs.\r\n#\r\n# Must be a durable file system that is accessible from all nodes\r\n# (like HDFS, S3, Ceph, nfs, ...)\r\n#\r\n# high-availability.storageDir: hdfs:///flink/ha/\r\nhigh-availability.storageDir: hdfs://x6mccluster:8020/flink/ha/\r\n\r\n# The list of ZooKeeper quorum peers that coordinate the high-availability\r\n# setup. This must be a list of the form:\r\n# \"host1:clientPort,host2:clientPort,...\" (default clientPort: 2181)\r\n#\r\n# high-availability.zookeeper.quorum: localhost:2181\r\nhigh-availability.zookeeper.quorum: hadoop001:2181,hadoop002:2181,hadoop003:2181\r\n\r\n# ACL options are based on https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#sc_BuiltinACLSchemes\r\n# It can be either \"creator\" (ZOO_CREATE_ALL_ACL) or \"open\" (ZOO_OPEN_ACL_UNSAFE)\r\n# The default value is \"open\" and it can be changed to \"creator\" if ZK security is enabled\r\n#\r\n# high-availability.zookeeper.client.acl: open\r\n\r\n#==============================================================================\r\n# Fault tolerance and checkpointing\r\n#==============================================================================\r\n\r\n# The backend that will be used to store operator state checkpoints if\r\n# checkpointing is enabled.\r\n#\r\n# Supported backends are \'jobmanager\', \'filesystem\', \'rocksdb\', or the\r\n# <class-name-of-factory>.\r\n#\r\n# state.backend: filesystem\r\nstate.backend: filesystem\r\n\r\n# Directory for checkpoints filesystem, when using any of the default bundled\r\n# state backends.\r\n#\r\n# state.checkpoints.dir: hdfs://namenode-host:port/flink-checkpoints\r\nstate.checkpoints.dir: hdfs://x6mccluster:8020/flink/checkpoints\r\n\r\n\r\n# Default target directory for savepoints, optional.\r\n#\r\n# state.savepoints.dir: hdfs://namenode-host:port/flink-savepoints\r\nstate.savepoints.dir: hdfs://x6mccluster:8020/flink/savepoints\r\n\r\n# Flag to enable/disable incremental checkpoints for backends that\r\n# support incremental checkpoints (like the RocksDB state backend).\r\n#\r\n# state.backend.incremental: false\r\n\r\n# The failover strategy, i.e., how the job computation recovers from task failures.\r\n# Only restart tasks that may have been affected by the task failure, which typically includes\r\n# downstream tasks and potentially upstream tasks if their produced data is no longer available for consumption.\r\n\r\njobmanager.execution.failover-strategy: region\r\n\r\n#==============================================================================\r\n# Rest & web frontend\r\n#==============================================================================\r\n\r\n# The port to which the REST client connects to. If rest.bind-port has\r\n# not been specified, then the server will bind to this port as well.\r\n#\r\n#rest.port: 8081\r\n\r\n# The address to which the REST client will connect to\r\n#\r\n#rest.address: 0.0.0.0\r\n\r\n# Port range for the REST and web server to bind to.\r\n#\r\n#rest.bind-port: 8080-8090\r\n\r\n# The address that the REST & web server binds to\r\n#\r\n#rest.bind-address: 0.0.0.0\r\n\r\n# Flag to specify whether job submission is enabled from the web-based\r\n# runtime monitor. Uncomment to disable.\r\n\r\n#web.submit.enable: false\r\n\r\n#==============================================================================\r\n# Advanced\r\n#==============================================================================\r\n\r\n# Override the directories for temporary files. If not specified, the\r\n# system-specific Java temporary directory (java.io.tmpdir property) is taken.\r\n#\r\n# For framework setups on Yarn or Mesos, Flink will automatically pick up the\r\n# containers\' temp directories without any need for configuration.\r\n#\r\n# Add a delimited list for multiple directories, using the system directory\r\n# delimiter (colon \':\' on unix) or a comma, e.g.:\r\n#     /data1/tmp:/data2/tmp:/data3/tmp\r\n#\r\n# Note: Each directory entry is read from and written to by a different I/O\r\n# thread. You can include the same directory multiple times in order to create\r\n# multiple I/O threads against that directory. This is for example relevant for\r\n# high-throughput RAIDs.\r\n#\r\n# io.tmp.dirs: /tmp\r\n\r\n# The classloading resolve order. Possible values are \'child-first\' (Flink\'s default)\r\n# and \'parent-first\' (Java\'s default).\r\n#\r\n# Child first classloading allows users to use different dependency/library\r\n# versions in their application than those in the classpath. Switching back\r\n# to \'parent-first\' may help with debugging dependency issues.\r\n#\r\n# classloader.resolve-order: child-first\r\n\r\n# The amount of memory going to the network stack. These numbers usually need\r\n# no tuning. Adjusting them may be necessary in case of an \"Insufficient number\r\n# of network buffers\" error. The default min is 64MB, the default max is 1GB.\r\n#\r\n# taskmanager.memory.network.fraction: 0.1\r\n# taskmanager.memory.network.min: 64mb\r\n# taskmanager.memory.network.max: 1gb\r\n\r\n#==============================================================================\r\n# Flink Cluster Security Configuration\r\n#==============================================================================\r\n\r\n# Kerberos authentication for various components - Hadoop, ZooKeeper, and connectors -\r\n# may be enabled in four steps:\r\n# 1. configure the local krb5.conf file\r\n# 2. provide Kerberos credentials (either a keytab or a ticket cache w/ kinit)\r\n# 3. make the credentials available to various JAAS login contexts\r\n# 4. configure the connector to use JAAS/SASL\r\n\r\n# The below configure how Kerberos credentials are provided. A keytab will be used instead of\r\n# a ticket cache if the keytab path and principal are set.\r\n\r\n# security.kerberos.login.use-ticket-cache: true\r\n# security.kerberos.login.keytab: /path/to/kerberos/keytab\r\n# security.kerberos.login.principal: flink-user\r\n\r\n# The configuration below defines which JAAS login contexts\r\n\r\n# security.kerberos.login.contexts: Client,KafkaClient\r\n\r\n#==============================================================================\r\n# ZK Security Configuration\r\n#==============================================================================\r\n\r\n# Below configurations are applicable if ZK ensemble is configured for security\r\n\r\n# Override below configuration to provide custom ZK service name if configured\r\n# zookeeper.sasl.service-name: zookeeper\r\n\r\n# The configuration below must match one of the values set in \"security.kerberos.login.contexts\"\r\n# zookeeper.sasl.login-context-name: Client\r\n\r\n#==============================================================================\r\n# HistoryServer\r\n#==============================================================================\r\n\r\n# The HistoryServer is started and stopped via bin/historyserver.sh (start|stop)\r\n\r\n# Directory to upload completed jobs to. Add this directory to the list of\r\n# monitored directories of the HistoryServer as well (see below).\r\njobmanager.archive.fs.dir: hdfs://x6mccluster:8020/flink/completed-jobs/\r\n\r\n# The address under which the web-based HistoryServer listens.\r\nhistoryserver.web.address: hadoop003\r\n\r\n# The port under which the web-based HistoryServer listens.\r\nhistoryserver.web.port: 8082\r\n\r\n# Comma separated list of directories to monitor for completed jobs.\r\nhistoryserver.archive.fs.dir: hdfs://x6mccluster:8020/flink/completed-jobs/\r\n\r\n# Interval in milliseconds for refreshing the monitored directories.\r\nhistoryserver.archive.fs.refresh-interval: 10000\r\n\r\n#==============================================================================\r\n# flink-web\r\n#==============================================================================\r\nclassloader.resolve-order: parent-first\r\n```\r\n**workers**\r\n```\r\nhadoop001\r\nhadoop002\r\nhadoop003\r\n```\r\n\r\n**masters**\r\n```\r\nhadoop002\r\nhadoop003\r\n```\r\n## 分发至所有机器\r\n```\r\n[hadoop@test-hadoop003 flink-1.13.5]$ xsync conf/\r\n```\r\n## 启动HA集群\r\n```\r\n[hadoop@test-hadoop003 bin]$ ./start-cluster.sh\r\n```\r\n```\r\n遇到下面的问题，停掉集群./stop-cluster.sh，然后到对应机器的/tmp目录下删除对应的pid文件\r\n[INFO] 1 instance(s) of taskexecutor are already running on node-1.hadoop.com.\r\n```\r\n## Flink on Yarn启动模式\r\nhttps://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/resource-providers/yarn/\r\n## 下载所需要的依赖\r\nhttps://repo.maven.apache.org/maven2/org/apache/flink/\r\n","timestamp":1649836682854},{"name":"21-安装flink-streaming-platform-web.md","path":"001-大数据/01-搭建hadoop平台/2-安装hadoop环境/21-安装flink-streaming-platform-web.md","content":"# <font color=#C71585>安装flink-streaming-platform-web</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## 下载安装包\r\nhttps://github.com/zhp8341/flink-streaming-platform-web/releases/tag/tagV20211211(flink1.13.2)\r\n## flink-cdc2.1下载\r\nhttps://ververica.github.io/flink-cdc-connectors/release-2.1/content/%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/index.html\r\n## 解压安装包\r\n```\r\n[hadoop@test-hadoop003 software]$ tar -xzvf /share/opt/software/flink-streaming-platform-web.tar.gz -C /share/opt/module/\r\n```\r\n## 新建数据库flink-web并初始化表\r\n**flink-web.sql**\r\n```sql\r\nSET NAMES utf8mb4;\r\nSET FOREIGN_KEY_CHECKS = 0;\r\n\r\n-- ----------------------------\r\n-- Table structure for alart_log\r\n-- ----------------------------\r\nDROP TABLE IF EXISTS `alart_log`;\r\nCREATE TABLE `alart_log` (\r\n  `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT,\r\n  `job_config_id` bigint(11) NOT NULL COMMENT \'job_config的id  如果0代表的是测试,\',\r\n  `job_name` varchar(255) DEFAULT NULL,\r\n  `message` varchar(512) DEFAULT NULL COMMENT \'消息内容\',\r\n  `type` tinyint(1) NOT NULL COMMENT \'1:钉钉\',\r\n  `status` tinyint(1) NOT NULL COMMENT \'1:成功 0:失败\',\r\n  `fail_log` text COMMENT \'失败原因\',\r\n  `is_deleted` tinyint(1) NOT NULL DEFAULT \'0\',\r\n  `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT \'创建时间\',\r\n  `edit_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \'修改时间\',\r\n  `creator` varchar(32) DEFAULT \'sys\',\r\n  `editor` varchar(32) DEFAULT \'sys\',\r\n  PRIMARY KEY (`id`),\r\n  KEY `index_job_config_id` (`job_config_id`) USING BTREE\r\n) ENGINE=InnoDB  DEFAULT CHARSET=utf8mb4 COMMENT=\'告警发送情况日志\';\r\n\r\n-- ----------------------------\r\n-- Table structure for ip_status\r\n-- ----------------------------\r\nDROP TABLE IF EXISTS `ip_status`;\r\nCREATE TABLE `ip_status` (\r\n  `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT,\r\n  `ip` varchar(64) NOT NULL COMMENT \'ip\',\r\n  `status` int(11) NOT NULL COMMENT \'1:运行 -1:停止 \',\r\n  `last_time` datetime DEFAULT NULL COMMENT \'最后心跳时间\',\r\n  `is_deleted` tinyint(1) NOT NULL DEFAULT \'0\',\r\n  `create_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT \'创建时间\',\r\n  `edit_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \'修改时间\',\r\n  `creator` varchar(32) NOT NULL DEFAULT \'sys\',\r\n  `editor` varchar(32) NOT NULL DEFAULT \'sys\',\r\n  PRIMARY KEY (`id`),\r\n  UNIQUE KEY `index_uk_ip` (`ip`) USING BTREE\r\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT=\'web服务运行ip\';\r\n\r\n-- ----------------------------\r\n-- Table structure for job_config\r\n-- ----------------------------\r\nDROP TABLE IF EXISTS `job_config`;\r\nCREATE TABLE `job_config` (\r\n  `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT,\r\n  `job_name` varchar(64) NOT NULL COMMENT \'任务名称\',\r\n  `deploy_mode` varchar(64) NOT NULL COMMENT \'提交模式: standalone 、yarn 、yarn-session \',\r\n  `flink_run_config` varchar(512) NOT NULL COMMENT \'flink运行配置\',\r\n  `flink_sql` MEDIUMTEXT NOT NULL COMMENT \'sql语句\',\r\n  `flink_checkpoint_config` varchar(512) DEFAULT NULL COMMENT \'checkPoint配置\',\r\n  `job_id` varchar(64) DEFAULT NULL COMMENT \'运行后的任务id\',\r\n  `is_open` tinyint(1) NOT NULL COMMENT \'1:开启 0: 关闭\',\r\n  `status` tinyint(1) NOT NULL COMMENT \'1:运行中 0: 停止中 -1:运行失败\',\r\n  `ext_jar_path` varchar(2048) DEFAULT NULL COMMENT \'udf地址已经连接器jar 如http://xxx.xxx.com/flink-streaming-udf.jar\',\r\n  `last_start_time` datetime DEFAULT NULL COMMENT \'最后一次启动时间\',\r\n  `last_run_log_id` bigint(11) DEFAULT NULL COMMENT \'最后一次日志\',\r\n  `version` int(11) NOT NULL DEFAULT \'0\' COMMENT \'更新版本号 用于乐观锁\',\r\n  `is_deleted` tinyint(1) NOT NULL DEFAULT \'0\',\r\n  `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT \'创建时间\',\r\n  `edit_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \'修改时间\',\r\n  `creator` varchar(32) DEFAULT \'sys\',\r\n  `editor` varchar(32) DEFAULT \'sys\',\r\n  PRIMARY KEY (`id`),\r\n  KEY `uk_index` (`job_name`) USING BTREE\r\n) ENGINE=InnoDB  DEFAULT CHARSET=utf8mb4 COMMENT=\'flink任务配置表\';\r\n\r\nALTER TABLE job_config add `job_type` tinyint(1) NOT NULL DEFAULT \'0\' COMMENT \'任务类型 0:sql 1:自定义jar\' AFTER version ;\r\nALTER TABLE job_config add `custom_args` varchar(128)  DEFAULT NULL COMMENT \'启动jar可能需要使用的自定义参数\' AFTER job_type;\r\nALTER TABLE job_config add `custom_main_class` varchar(128)  DEFAULT NULL COMMENT \'程序入口类\' AFTER custom_args;\r\nALTER TABLE job_config add `custom_jar_url` varchar(128)  DEFAULT NULL   COMMENT\'自定义jar的http地址 如:http://ccblog.cn/xx.jar\' AFTER custom_main_class;\r\n\r\n\r\n-- ----------------------------\r\n-- Table structure for job_config_history\r\n-- ----------------------------\r\nCREATE TABLE `job_config_history` (\r\n                                      `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT,\r\n                                      `job_config_id` bigint(11) NOT NULL COMMENT \'job_config主表Id\',\r\n                                      `job_name` varchar(64) NOT NULL COMMENT \'任务名称\',\r\n                                      `deploy_mode` varchar(64) NOT NULL COMMENT \'提交模式: standalone 、yarn 、yarn-session \',\r\n                                      `flink_run_config` varchar(512) NOT NULL COMMENT \'flink运行配置\',\r\n                                      `flink_sql` mediumtext NOT NULL COMMENT \'sql语句\',\r\n                                      `flink_checkpoint_config` varchar(512) DEFAULT NULL COMMENT \'checkPoint配置\',\r\n                                      `ext_jar_path` varchar(2048) DEFAULT NULL COMMENT \'udf地址及连接器jar 如http://xxx.xxx.com/flink-streaming-udf.jar\',\r\n                                      `version` int(11) NOT NULL DEFAULT \'0\' COMMENT \'更新版本号\',\r\n                                      `is_deleted` tinyint(1) NOT NULL DEFAULT \'0\',\r\n                                      `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT \'创建时间\',\r\n                                      `edit_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \'修改时间\',\r\n                                      `creator` varchar(32) DEFAULT \'sys\',\r\n                                      `editor` varchar(32) DEFAULT \'sys\',\r\n                                      PRIMARY KEY (`id`),\r\n                                      KEY `index_job_config_id` (`job_config_id`) USING BTREE\r\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT=\'flink任务配置历史变更表\';\r\n\r\n\r\n\r\n-- ----------------------------\r\n-- Table structure for job_run_log\r\n-- ----------------------------\r\nDROP TABLE IF EXISTS `job_run_log`;\r\nCREATE TABLE `job_run_log` (\r\n  `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT,\r\n  `job_config_id` bigint(11) NOT NULL,\r\n  `job_name` varchar(64) NOT NULL COMMENT \'任务名称\',\r\n  `deploy_mode` varchar(64) NOT NULL COMMENT \'提交模式: standalone 、yarn 、yarn-session \',\r\n  `job_id` varchar(64) DEFAULT NULL COMMENT \'运行后的任务id\',\r\n  `local_log` mediumtext COMMENT \'启动时本地日志\',\r\n  `remote_log_url` varchar(128) DEFAULT NULL COMMENT \'远程日志url的地址\',\r\n  `start_time` datetime DEFAULT NULL COMMENT \'启动时间\',\r\n  `end_time` datetime DEFAULT NULL COMMENT \'启动时间\',\r\n  `job_status` varchar(32) DEFAULT NULL COMMENT \'任务状态\',\r\n  `is_deleted` tinyint(1) NOT NULL DEFAULT \'0\',\r\n  `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT \'创建时间\',\r\n  `edit_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \'修改时间\',\r\n  `creator` varchar(32) DEFAULT \'sys\',\r\n  `editor` varchar(32) DEFAULT \'sys\',\r\n  PRIMARY KEY (`id`)\r\n) ENGINE=InnoDB  DEFAULT CHARSET=utf8mb4 COMMENT=\'运行任务日志\';\r\n\r\nALTER TABLE job_run_log add `run_ip`  varchar(64) DEFAULT NULL COMMENT \'任务运行所在的机器\' AFTER local_log ;\r\n\r\n\r\n-- ----------------------------\r\n-- Table structure for savepoint_backup\r\n-- ----------------------------\r\nDROP TABLE IF EXISTS `savepoint_backup`;\r\nCREATE TABLE `savepoint_backup` (\r\n  `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT,\r\n  `job_config_id` bigint(11) NOT NULL,\r\n  `savepoint_path` varchar(2048) NOT NULL COMMENT \'地址\',\r\n  `backup_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \'备份时间\',\r\n  `is_deleted` tinyint(1) NOT NULL DEFAULT \'0\',\r\n  `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT \'创建时间\',\r\n  `edit_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \'修改时间\',\r\n  `creator` varchar(32) DEFAULT \'sys\',\r\n  `editor` varchar(32) DEFAULT \'sys\',\r\n  PRIMARY KEY (`id`),\r\n  KEY `index` (`job_config_id`) USING BTREE\r\n) ENGINE=InnoDB  DEFAULT CHARSET=utf8mb4 COMMENT=\'savepoint备份地址\';\r\n\r\n\r\n\r\n-- ----------------------------\r\n-- Table structure for system_config\r\n-- ----------------------------\r\nDROP TABLE IF EXISTS `system_config`;\r\nCREATE TABLE `system_config` (\r\n  `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT,\r\n  `key` varchar(128) NOT NULL COMMENT \'key值\',\r\n  `val` varchar(512) NOT NULL COMMENT \'value\',\r\n  `type` varchar(12) NOT NULL COMMENT \'类型 如:sys  alarm\',\r\n  `is_deleted` tinyint(1) NOT NULL DEFAULT \'0\',\r\n  `create_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT \'创建时间\',\r\n  `edit_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \'修改时间\',\r\n  `creator` varchar(32) NOT NULL DEFAULT \'sys\',\r\n  `editor` varchar(32) NOT NULL DEFAULT \'sys\',\r\n  PRIMARY KEY (`id`)\r\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT=\'系统配置\';\r\n\r\n\r\n\r\n-- ----------------------------\r\n-- Table structure for user\r\n-- ----------------------------\r\nDROP TABLE IF EXISTS `user`;\r\nCREATE TABLE `user` (\r\n  `id` int(11) NOT NULL AUTO_INCREMENT,\r\n  `username` varchar(255) COLLATE utf8mb4_bin DEFAULT NULL COMMENT \'用户名\',\r\n  `password` varchar(255) COLLATE utf8mb4_bin DEFAULT NULL COMMENT \'密码\',\r\n  `stauts` tinyint(1) NOT NULL COMMENT \'1:启用 0: 停用\',\r\n  `is_deleted` tinyint(1) NOT NULL DEFAULT \'0\' COMMENT \'1:删除 0: 未删除\',\r\n  `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT \'创建时间\',\r\n  `edit_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \'修改时间\',\r\n  `creator` varchar(32) COLLATE utf8mb4_bin DEFAULT \'sys\',\r\n  `editor` varchar(32) COLLATE utf8mb4_bin DEFAULT \'sys\',\r\n  PRIMARY KEY (`id`),\r\n  UNIQUE KEY `index_uk` (`username`) USING BTREE\r\n) ENGINE=InnoDB  DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin;\r\n\r\n\r\n\r\nCREATE TABLE `job_alarm_config`\r\n(\r\n    `id`          bigint(11) unsigned NOT NULL AUTO_INCREMENT,\r\n    `job_id`      bigint(11) unsigned NOT NULL COMMENT \'job_config主表id\',\r\n    `type`        tinyint(1)          NOT NULL COMMENT \'类型 1:钉钉告警 2:url回调 3:异常自动拉起任务\',\r\n    `version`     int(11)             NOT NULL DEFAULT \'0\' COMMENT \'更新版本号  \',\r\n    `is_deleted`  tinyint(1)          NOT NULL DEFAULT \'0\',\r\n    `create_time` datetime                     DEFAULT CURRENT_TIMESTAMP COMMENT \'创建时间\',\r\n    `edit_time`   datetime                     DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \'修改时间\',\r\n    `creator`     varchar(32)                  DEFAULT \'sys\',\r\n    `editor`      varchar(32)                  DEFAULT \'sys\',\r\n    PRIMARY KEY (`id`),\r\n    KEY `uk_index_job_id` (`job_id`) USING BTREE\r\n) ENGINE = InnoDB\r\n  DEFAULT CHARSET = utf8mb4 COMMENT =\'告警辅助配置表\';\r\n\r\n-- ----------------------------\r\n-- Records of user 默认密码是 123456\r\n-- ----------------------------\r\nBEGIN;\r\nINSERT INTO `user` VALUES (1, \'admin\', \'e10adc3949ba59abbe56e057f20f883e\', 1, 0, \'2020-07-10 22:15:04\', \'2020-07-24 22:21:35\', \'sys\', \'sys\');\r\nCOMMIT;\r\n\r\nSET FOREIGN_KEY_CHECKS = 1;\r\n```\r\n## 修改配置文件\r\n```\r\n[hadoop@test-hadoop003 conf]$ vim application.properties\r\n```\r\n## 启动web服务\r\n```\r\n[hadoop@test-hadoop003 bin]$ sh deploy.sh start\r\n```\r\n","timestamp":1649836682854},{"name":"01-hadoop初始化启动.md","path":"001-大数据/01-搭建hadoop平台/3-启动hadoop/01-hadoop初始化启动.md","content":"# <font color=#C71585>hadoop初始化启动</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## 启动三个节点的zookeeper服务\r\n```\r\n[hadoop@hadoop001 module]$ ./zookeeper/bin/zkServer.sh start\r\n[hadoop@hadoop002 module]$ ./zookeeper/bin/zkServer.sh start\r\n[hadoop@hadoop003 module]$ ./zookeeper/bin/zkServer.sh start\r\n```\r\n**在三台机器上jps可以看到QuorumPeerMain服务进程**\r\n\r\n*并查看日志:*\r\n```\r\n\t[hadoop@hadoop001 module]$ /opt/module/zookeeper/logs/zookeeper-hadoop-server-hadoop001.out\r\n\t[hadoop@hadoop002 module]$ /opt/module/zookeeper/logs/zookeeper-hadoop-server-hadoop002.out\r\n\t[hadoop@hadoop003 module]$ /opt/module/zookeeper/logs/zookeeper-hadoop-server-hadoop003.out\r\n```\r\n## 创建命名空间，主namenode节点执行\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/bin/hdfs zkfc -formatZK\r\n```\r\n## 三个节点用如下命令启动日志程序\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/sbin/hadoop-daemon.sh start journalnode\r\n[hadoop@hadoop002 module]$ ./hadoop/sbin/hadoop-daemon.sh start journalnode\r\n[hadoop@hadoop003 module]$ ./hadoop/sbin/hadoop-daemon.sh start journalnode\r\n```\r\n**在三台机器上jps可以看到JournalNode服务进程**\r\n\r\n*并查看日志:*\r\n```\r\n\t[hadoop@hadoop001 module]$ /opt/module/hadoop/logs/hadoop-hadoop-journalnode-hadoop001.log\r\n\t[hadoop@hadoop002 module]$ /opt/module/hadoop/logs/hadoop-hadoop-journalnode-hadoop002.log\r\n\t[hadoop@hadoop003 module]$ /opt/module/hadoop/logs/hadoop-hadoop-journalnode-hadoop003.log\r\n```\r\n## 主namenode节点格式化namenode和journalnode目录\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/bin/hdfs namenode -format\r\n```\r\n## 主namenode节点启动namenode进程\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/sbin/hadoop-daemon.sh start namenode\r\n```\r\n**在hadoop001上jps可以看到NameNode服务进程**\r\n\r\n*并查看日志:*\r\n```\r\n\t[hadoop@hadoop001 module]$ /opt/module/hadoop/logs/hadoop-hadoop-namenode-hadoop001.log\r\n```\r\n## 在备namenode节点执行第一行命令，把备namenode节点的目录格式化并把元数据从主namenode节点copy过来，并且这个命令不会把journalnode目录再格式化了！然后用第二个命令启动备namenode进程！\r\n```\r\n[hadoop@hadoop002 module]$ ./hadoop/bin/hdfs namenode -bootstrapStandby\r\n[hadoop@hadoop002 module]$ ./hadoop/sbin/hadoop-daemon.sh start namenode\r\n```\r\n**在hadoop002上jps可以看到NameNode服务进程**\r\n\r\n*并查看日志:*\r\n```\r\n\t[hadoop@hadoop002 module]$ /opt/module/hadoop/logs/hadoop-hadoop-namenode-hadoop002.log\r\n```\r\n## 两个namenode节点都执行以下命令\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/sbin/hadoop-daemon.sh start zkfc\r\n[hadoop@hadoop002 module]$ ./hadoop/sbin/hadoop-daemon.sh start zkfc\r\n```\r\n**在hadoop001/hadoop002上jps可以看到DFSZKFailoverController服务进程**\r\n\r\n*并查看日志:*\r\n```\r\n\t[hadoop@hadoop001 module]$ /opt/module/hadoop/logs/hadoop-hadoop-zkfc-hadoop001.log\r\n\t[hadoop@hadoop002 module]$ /opt/module/hadoop/logs/hadoop-hadoop-zkfc-hadoop002.log\r\n```\r\n## 启动datanode,在所有DataNode节点单独启动\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/sbin/hadoop-daemon.sh start datanode\r\n[hadoop@hadoop002 module]$ ./hadoop/sbin/hadoop-daemon.sh start datanode\r\n[hadoop@hadoop003 module]$ ./hadoop/sbin/hadoop-daemon.sh start datanode\r\n```\r\n**在hadoop001/hadoop002/hadoop003上jps可以看到DataNode服务进程**\r\n\r\n*并查看日志:*\r\n```\r\n\t[hadoop@hadoop001 module]$ /opt/module/hadoop/logs/hadoop-hadoop-datanode-hadoop001.log\r\n\t[hadoop@hadoop002 module]$ /opt/module/hadoop/logs/hadoop-hadoop-datanode-hadoop002.log\r\n\t[hadoop@hadoop003 module]$ /opt/module/hadoop/logs/hadoop-hadoop-datanode-hadoop003.log\r\n```\r\n## 启动Yarn和备ResourceManager，主NameNode节点\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/sbin/start-yarn.sh\r\n```\r\n\r\n**会启动hadoop001、hadoop002的ResourceManager，以及hadoop001/hadoop002/hadoop003的NodeManager**\r\n\r\n**若启动失败，则单独启动备节点ResourceManager**\r\n```\r\n\t./hadoop/sbin/yarn-daemon.sh start resourcemanager\r\n```\r\n**启动所有节点的nodemanager**\r\n```\r\n\t./hadoop/sbin/yarn-daemon.sh start nodemanager\r\n```\r\n\r\n*并查看日志:*\r\n```\r\n\t[hadoop@hadoop001 module]$ /opt/module/hadoop/logs/hadoop-hadoop-resourcemanager-hadoop001.log\r\n\t[hadoop@hadoop001 module]$ /opt/module/hadoop/logs/hadoop-hadoop-nodemanager-hadoop001.log\r\n\t[hadoop@hadoop002 module]$ /opt/module/hadoop/logs/hadoop-hadoop-resourcemanager-hadoop002.log\r\n\t[hadoop@hadoop002 module]$ /opt/module/hadoop/logs/hadoop-hadoop-nodemanager-hadoop002.log\r\n\t[hadoop@hadoop003 module]$ /opt/module/hadoop/logs/hadoop-hadoop-nodemanager-hadoop003.log\r\n```\r\n## 在hadoop003启动历史服务器\r\n```\r\n[hadoop@hadoop003 module]$ hadoop/bin/mapred --daemon start historyserver\r\n```\r\n**会启动hadoop003的JobHistoryServer**\r\n\r\n*并查看日志:*\r\n```\r\n\t[hadoop@hadoop003 module]$ /opt/module/hadoop/logs/hadoop-hadoop-historyserver-hadoop003.log\r\n```\r\n## 查看namenode和resourcemanager主从状态\r\n```\r\n[hadoop@hadoop001 module]$ hdfs haadmin -getServiceState nn1\r\n[hadoop@hadoop001 module]$ hdfs haadmin -getServiceState nn2\r\n[hadoop@hadoop001 module]$ yarn rmadmin -getServiceState rm1\r\n[hadoop@hadoop001 module]$ yarn rmadmin -getServiceState rm2\r\n```\r\n**Yarn任务管理webUI界面：** http://hadoop001:8188/ 或者 http://hadoop002:8188/\r\n\r\n**Namenode管理webUI界面：** http://hadoop001:9870/  和  http://hadoop002:9870/\r\n\r\n**historyserver管理webUI界面：** http://hadoop003:19888/jobhistory\r\n","timestamp":1649836682854},{"name":"02-hive初始化启动.md","path":"001-大数据/01-搭建hadoop平台/3-启动hadoop/02-hive初始化启动.md","content":"# <font color=#C71585>hive初始化启动</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## 在hdfs上创建好hive存储数据的目录\r\n```\r\n[hadoop@hadoop001 module]$ hadoop/bin/hdfs dfs -mkdir /spark\r\n[hadoop@hadoop001 module]$ hadoop/bin/hdfs dfs -mkdir /spark/spark-jars\r\n[hadoop@hadoop001 module]$ hadoop/bin/hdfs dfs -mkdir /spark/spark-history\r\n[hadoop@hadoop001 module]$ hadoop/bin/hdfs dfs -mkdir /tmp\r\n[hadoop@hadoop001 module]$ hadoop/bin/hdfs dfs -mkdir -p /user/hive/warehouse\r\n[hadoop@hadoop001 module]$ hadoop/bin/hdfs dfs -mkdir -p /spark/spark-log\r\n[hadoop@hadoop001 module]$ hadoop/bin/hdfs dfs -mkdir -p /tez\r\n\r\n\r\n[hadoop@hadoop001 module]$ hadoop/bin/hdfs dfs -chmod g+w /spark\r\n[hadoop@hadoop001 module]$ hadoop/bin/hdfs dfs -chmod g+w /spark/spark-jars\r\n[hadoop@hadoop001 module]$ hadoop/bin/hdfs dfs -chmod g+w /spark/spark-history\r\n[hadoop@hadoop001 module]$ hadoop/bin/hdfs dfs -chmod g+w /tmp\r\n[hadoop@hadoop001 module]$ hadoop/bin/hdfs dfs -chmod g+w /user/hive/warehouse\r\n[hadoop@hadoop001 module]$ hadoop/bin/hdfs dfs -chmod g+w /spark/spark-log\r\n[hadoop@hadoop001 module]$ hadoop/bin/hdfs dfs -chmod g+w /tez\r\n\r\n[hadoop@hadoop001 module]$ hadoop/bin/hdfs dfs -put lib_all/spark-jars/* /spark/spark-jars/\r\n[hadoop@hadoop001 module]$ hadoop/bin/hdfs dfs -put tez/tez.tar.gz /tez/\r\n```\r\n## 初始化hive元数据库\r\n```\r\n[hadoop@hadoop002 module]$ ./hive/bin/schematool -dbType mysql -initSchema\r\n```\r\n## hive 远程服务启动方式[metastore和hiveserver2在同一台上启动即可]，暂时设定第二台启动\r\n```\r\n[hadoop@hadoop002 module]$ cd hive/logs\r\n[hadoop@hadoop002 logs]$ nohup hive --service metastore &> metastore.log &\r\n[hadoop@hadoop002 logs]$ nohup hive --service hiveserver2 &> hiveserver2.log &\r\n```\r\n**查看hive启动日志**\r\n```\r\n\t[hadoop@hadoop002 module]$ vim hive/logs/hive.log\r\n```\r\n","timestamp":1649836682854},{"name":"03-停止hadoop集群.md","path":"001-大数据/01-搭建hadoop平台/3-启动hadoop/03-停止hadoop集群.md","content":"# <font color=#C71585>停止hadoop集群</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## 停止hadoop001的hue服务\r\n```\r\n[hadoop@hadoop001 bin]$ pwd\r\n/opt/module/hue/bin\r\n[hadoop@hadoop001 bin]$ ps -ef | grep hue\r\n[hadoop@hadoop001 bin]$ kill -9 32000 32001\r\n```\r\n## 停止hadoop001的HiveMetaStore、hadoop002的HiveMetaStore和HiveServer2\r\n```\r\n[hadoop@hadoop001 bin]$ pwd\r\n/opt/module/hive/bin\r\n[hadoop@hadoop001 bin]$ ps -ef | grep hive\r\n[hadoop@hadoop001 bin]$ kill -9 32000\r\n[hadoop@hadoop002 bin]$ pwd\r\n/opt/module/hive/bin\r\n[hadoop@hadoop002 bin]$ ps -ef | grep hive\r\n[hadoop@hadoop002 bin]$ kill -9 14433 14623\r\n```\r\n## 在hadoop001停止历史服务器\r\n```\r\n[hadoop@hadoop001 module]$ cd hadoop/sbin/\r\n[hadoop@hadoop001 sbin]$ mapred --daemon stop historyserver\r\n```\r\n## 停止Yarn和备ResourceManager，主NameNode节点\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/sbin/stop-yarn.sh\r\n```\r\n## 关闭datanode,在所有DataNode节点单独关闭\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/sbin/hadoop-daemon.sh stop datanode\r\n[hadoop@hadoop002 module]$ ./hadoop/sbin/hadoop-daemon.sh stop datanode\r\n[hadoop@hadoop004 module]$ ./hadoop/sbin/hadoop-daemon.sh stop datanode\r\n```\r\n## 两个namenode节点都执行以下命令\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/sbin/hadoop-daemon.sh stop zkfc\r\n[hadoop@hadoop002 module]$ ./hadoop/sbin/hadoop-daemon.sh stop zkfc\r\n```\r\n## 停止namenode节点\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/sbin/hadoop-daemon.sh stop namenode\r\n[hadoop@hadoop002 module]$ ./hadoop/sbin/hadoop-daemon.sh stop namenode\r\n```\r\n## 停止日志程序\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/sbin/hadoop-daemon.sh stop journalnode\r\n[hadoop@hadoop002 module]$ ./hadoop/sbin/hadoop-daemon.sh stop journalnode\r\n[hadoop@hadoop004 module]$ ./hadoop/sbin/hadoop-daemon.sh stop journalnode\r\n```\r\n","timestamp":1649836682854},{"name":"04-启动hadoop集群.md","path":"001-大数据/01-搭建hadoop平台/3-启动hadoop/04-启动hadoop集群.md","content":"# <font color=#C71585>启动hadoop集群</font>\r\n>维护人员：**高俊**  \r\n>创建时间：2022-04-12\r\n\r\n## 启动日志程序\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/sbin/hadoop-daemon.sh start journalnode\r\n[hadoop@hadoop002 module]$ ./hadoop/sbin/hadoop-daemon.sh start journalnode\r\n[hadoop@hadoop004 module]$ ./hadoop/sbin/hadoop-daemon.sh start journalnode\r\n```\r\n**在三台机器上jps可以看到JournalNode服务进程**\r\n\r\n*并查看日志:*\r\n```\r\n\t[hadoop@hadoop001 module]$ /opt/module/hadoop/logs/hadoop-hadoop-journalnode-hadoop001.log\r\n\t[hadoop@hadoop002 module]$ /opt/module/hadoop/logs/hadoop-hadoop-journalnode-hadoop002.log\r\n\t[hadoop@hadoop004 module]$ /opt/module/hadoop/logs/hadoop-hadoop-journalnode-hadoop004.log\r\n```\r\n## 启动namenode节点\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/sbin/hadoop-daemon.sh start namenode\r\n[hadoop@hadoop002 module]$ ./hadoop/sbin/hadoop-daemon.sh start namenode\r\n```\r\n**在hadoop001上jps可以看到NameNode服务进程**\r\n\r\n**在hadoop002上jps可以看到NameNode服务进程**\r\n\r\n*并查看日志:*\r\n```\r\n\t[hadoop@hadoop001 module]$ /opt/module/hadoop/logs/hadoop-hadoop-namenode-hadoop001.log\r\n\t[hadoop@hadoop002 module]$ /opt/module/hadoop/logs/hadoop-hadoop-namenode-hadoop002.log\r\n```\r\n## 启动两个namenode节点的zkfc\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/sbin/hadoop-daemon.sh start zkfc\r\n[hadoop@hadoop002 module]$ ./hadoop/sbin/hadoop-daemon.sh start zkfc\r\n```\r\n## 启动datanode,在所有DataNode节点单独启动\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/sbin/hadoop-daemon.sh start datanode\r\n[hadoop@hadoop002 module]$ ./hadoop/sbin/hadoop-daemon.sh start datanode\r\n[hadoop@hadoop004 module]$ ./hadoop/sbin/hadoop-daemon.sh start datanode\r\n```\r\n## 启动Yarn和备ResourceManager，主NameNode节点\r\n```\r\n[hadoop@hadoop001 module]$ ./hadoop/sbin/start-yarn.sh\r\n```\r\n## 在hadoop001启动历史服务器\r\n```\r\n[hadoop@hadoop001 module]$ cd hadoop/sbin/\r\n[hadoop@hadoop001 sbin]$ mapred --daemon start historyserver\r\n[hadoop@hadoop001 sbin]$ cd /opt/module/\r\n```\r\n## 启动hadoop002的HiveMetaStore和HiveServer2，hadoop001的HiveMetaStore\r\n```\r\n[hadoop@hadoop002 module]$ cd hive/logs\r\n[hadoop@hadoop002 logs]$ nohup hive --service metastore &> metastore.log &\r\n[hadoop@hadoop002 logs]$ nohup hive --service hiveserver2 &> hiveserver2.log &\r\n```\r\n*查看hive启动日志*\r\n```\r\n\t[hadoop@hadoop002 module]$ vim hive/logs/hive.log\r\n  [hadoop@hadoop001 module]$ cd hive/logs\r\n  [hadoop@hadoop001 logs]$ nohup hive --service metastore &> metastore.log &\r\n```\r\n*查看hive启动日志*\r\n```\r\n\t[hadoop@hadoop001 module]$ vim hive/logs/hive.log\r\n```\r\n## 启动hadoop001的hue服务\r\n```\r\n  [hadoop@hadoop001 logs]$ nohup /opt/module/hue/build/env/bin/supervisor &> hue.log &\r\n```\r\n","timestamp":1649836682854}]